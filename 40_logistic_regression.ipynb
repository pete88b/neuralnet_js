{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pursuant-mailman",
   "metadata": {},
   "source": [
    "# js implementation of Nick Beckers post\n",
    "\n",
    "- https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "- https://github.com/beckernick/logistic_regression_from_scratch/blob/master/logistic_regression_scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "japanese-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {shape,transpose,dotProduct,full,zeros} from './src/util.module.js';\n",
    "import {matrixSum1d,matrixSubtract1d,matrixMultiply1d} from './src/util.module.js';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heated-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/eval\n",
    "function looseJsonParse(obj){\n",
    "    return Function('\"use strict\";return (' + obj + ')')();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "const num_observations=5000;\n",
    "const x1=looseJsonParse(require('fs').readFileSync('data/x1.txt').toString());\n",
    "const x2=looseJsonParse(require('fs').readFileSync('data/x2.txt').toString());\n",
    "const simulated_separableish_features = [...x1,...x2];\n",
    "const simulated_labels = [...zeros(num_observations), ...full(num_observations,null,1)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affected-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid(scores) {\n",
    "    return scores.map(score => 1./(1.+Math.pow(Math.E, -score)));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radical-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "function exp(a) {\n",
    "    return Math.pow(Math.E, a);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suspected-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_likelihood(features, target, weights) {\n",
    "    let scores = dotProduct(features, transpose([weights]));\n",
    "    scores = transpose(scores)[0];\n",
    "    let temp = matrixSubtract1d(\n",
    "        matrixMultiply1d(target,scores),\n",
    "        scores.map(score=>Math.log(1+exp(score))))\n",
    "    return temp.reduce((a,b)=>a+b);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominican-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "function logistic_regression(features, target, num_steps, learning_rate, add_intercept) {\n",
    "    if (add_intercept) {\n",
    "        features=features.map(feature=>[1, ...feature]);\n",
    "    }\n",
    "        \n",
    "    let weights = zeros(shape(features)[1]);\n",
    "    \n",
    "    for (let step=0; step<num_steps; step++) {\n",
    "        let scores = dotProduct(features, transpose([weights]));\n",
    "        scores = transpose(scores)[0];\n",
    "        let predictions = sigmoid(scores);\n",
    "        \n",
    "        // Update weights with log likelihood gradient\n",
    "        let output_error_signal = matrixSubtract1d(target, predictions);\n",
    "        let gradient = dotProduct(transpose(features), transpose([output_error_signal]));\n",
    "        gradient=transpose(gradient)[0];\n",
    "        weights=matrixSum1d(weights, matrixMultiply1d(gradient,learning_rate))\n",
    "\n",
    "        // Print log-likelihood every so often\n",
    "        if (step % 100 == 0) {\n",
    "            console.log(step,weights,'log likelihood',log_likelihood(features, target, weights));\n",
    "        }\n",
    "    }\n",
    "    return weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-modification",
   "metadata": {},
   "source": [
    "Just a couple of changes from logistic_regression_scratch.ipynb ... I'm way too impatient to wait for 50000 epochs, so I train for less epochs at a higher learning rate. To get nearly the same loss and weights.\n",
    "\n",
    "We're targeting;\n",
    "- log likelihood of `-140.725421355` and\n",
    "- weights of\n",
    "    - `[-13.99400797] [[-5.02712572  8.23286799]]` sklearn LogisticRegression\n",
    "    - `[-14.09225541  -5.05899648   8.28955762]` logistic_regression_scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "educated-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0, 7.397425245475898, 30.01541021294583 ] log likelihood -72310.42400198299\n",
      "100 [ -17.82666273125494, -6.369924537519446, 10.445885059868809 ] log likelihood -146.29124937054002\n",
      "200 [ -17.317295390044116, -6.190795753701512, 10.151332603893513 ] log likelihood -145.0298937263822\n",
      "300 [ -16.85600559637878, -6.029144899861061, 9.884854139891312 ] log likelihood -143.99638603132615\n",
      "400 [ -16.442948712538392, -5.884368861535347, 9.646286962075505 ] log likelihood -143.1676362217944\n",
      "500 [ -16.077142806960314, -5.7561178277138, 9.43504911904683 ] log likelihood -142.51756242722146\n",
      "600 [ -15.75673684147998, -5.643745398976429, 9.250058841631924 ] log likelihood -142.01875219214884\n",
      "700 [ -15.479104810001434, -5.546337907261038, 9.08978876028366 ] log likelihood -141.64416106609727\n",
      "800 [ -15.241000821268164, -5.462766473478286, 8.952355636869779 ] log likelihood -141.36858429542463\n",
      "900 [ -15.03875298305558, -5.391753085164429, 8.835632322938354 ] log likelihood -141.16971380495966\n",
      "1000 [ -14.868468785251494, -5.331941309426124, 8.737366198662958 ] log likelihood -141.02870601865288\n",
      "1100 [ -14.726226698167942, -5.281962788053178, 8.655289539839904 ] log likelihood -140.93029631717906\n",
      "1200 [ -14.608235670894384, -5.2404929795824335, 8.587211293823463 ] log likelihood -140.86256957219484\n",
      "1300 [ -14.510953183779367, -5.206292704352739, 8.531084918755552 ] log likelihood -140.8165222803516\n",
      "1400 [ -14.431160740287964, -5.178234940246015, 8.485051678671818 ] log likelihood -140.78553936835561\n",
      "1500 [ -14.366001542320937, -5.155318409403761, 8.44746215336353 ] log likelihood -140.76487578869526\n",
      "1600 [ -14.312988178902255, -5.136670624757371, 8.416880489676375 ] log likelihood -140.7511962292148\n",
      "1700 [ -14.26998892312278, -5.121543373185132, 8.392076353313032 ] log likelihood -140.74219580108473\n",
      "1800 [ -14.235200477071382, -5.10930337464822, 8.372009099381136 ] log likelihood -140.7363040461986\n",
      "1900 [ -14.207113502804786, -5.099420345527255, 8.35580781142703 ] log likelihood -140.73246333079896\n"
     ]
    }
   ],
   "source": [
    "const weights = logistic_regression(simulated_separableish_features, simulated_labels, 2000, 3e-3, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
