{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "//default_exp rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-restaurant",
   "metadata": {},
   "source": [
    "# rnn\n",
    "\n",
    "> Implement some of the language models in https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-section",
   "metadata": {},
   "source": [
    "## Set-up data used in tests / demos\n",
    "    \n",
    "Using [human_numbers.tgz](https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz), I created an even smaller dataset with the following;\n",
    "\n",
    "```\n",
    "lines = []\n",
    "with open('data/human_numbers/train.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/valid.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/train_and_valid.txt','w') as f:\n",
    "    f.write(' . '.join([l.strip() for l in lines[:2000]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo\n"
     ]
    }
   ],
   "source": [
    "const text=require('fs').readFileSync('data/human_numbers/train_and_valid.txt').toString();\n",
    "text.substring(0,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadband-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  'one',      '.',        'two',\n",
      "  'three',    'four',     'five',\n",
      "  'six',      'seven',    'eight',\n",
      "  'nine',     'ten',      'eleven',\n",
      "  'twelve',   'thirteen', 'fourteen',\n",
      "  'fifteen',  'sixteen',  'seventeen',\n",
      "  'eighteen', 'nineteen', 'twenty',\n",
      "  'thirty',   'forty',    'fifty',\n",
      "  'sixty',    'seventy',  'eighty',\n",
      "  'ninety',   'hundred',  'thousand'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const tokens=text.split(' ');\n",
    "const vocab = [...new Set(tokens)];\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  one: 0,\n",
      "  '.': 1,\n",
      "  two: 2,\n",
      "  three: 3,\n",
      "  four: 4,\n",
      "  five: 5,\n",
      "  six: 6,\n",
      "  seven: 7,\n",
      "  eight: 8,\n",
      "  nine: 9,\n",
      "  ten: 10,\n",
      "  eleven: 11,\n",
      "  twelve: 12,\n",
      "  thirteen: 13,\n",
      "  fourteen: 14,\n",
      "  fifteen: 15,\n",
      "  sixteen: 16,\n",
      "  seventeen: 17,\n",
      "  eighteen: 18,\n",
      "  nineteen: 19,\n",
      "  twenty: 20,\n",
      "  thirty: 21,\n",
      "  forty: 22,\n",
      "  fifty: 23,\n",
      "  sixty: 24,\n",
      "  seventy: 25,\n",
      "  eighty: 26,\n",
      "  ninety: 27,\n",
      "  hundred: 28,\n",
      "  thousand: 29\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const word2idx={};\n",
    "vocab.forEach((word,idx)=>word2idx[word]=idx);\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dense-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   0,  1,  2,  1,  3, 1,  4,  1,  5,  1,  6, 1,\n",
      "   7,  1,  8,  1,  9, 1, 10,  1, 11,  1, 12, 1,\n",
      "  13,  1, 14,  1, 15, 1, 16,  1, 17,  1, 18, 1,\n",
      "  19,  1, 20,  1, 20, 0,  1, 20,  2,  1, 20, 3,\n",
      "   1, 20,  4,  1, 20, 5,  1, 20,  6,  1, 20, 7,\n",
      "   1, 20,  8,  1, 20, 9,  1, 21,  1, 21,  0, 1,\n",
      "  21,  2,  1, 21,  3, 1, 21,  4,  1, 21,  5, 1,\n",
      "  21,  6,  1, 21,  7, 1, 21,  8,  1, 21,  9, 1,\n",
      "  22,  1, 22,  0,\n",
      "  ... 10921 more items\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const nums=tokens.map(e=>word2idx[e]);\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similar-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostCommonToken [ '.', 1999 ] 0.18138099990926412\n"
     ]
    }
   ],
   "source": [
    "const tokenCounter={};\n",
    "vocab.forEach(token=>tokenCounter[token]=0);\n",
    "tokens.forEach(token=>tokenCounter[token]++);\n",
    "let mostCommonToken=[null,0];\n",
    "for (let key in tokenCounter) {\n",
    "    if (tokenCounter[key]>mostCommonToken[1]) {\n",
    "        mostCommonToken=[key,tokenCounter[key]];\n",
    "    }\n",
    "}\n",
    "console.log('mostCommonToken', mostCommonToken, mostCommonToken[1]/tokens.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-easter",
   "metadata": {},
   "source": [
    "&uarr; if we can get better than 0.18 accuracy, our model will be doing better than predicting the most common token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "guided-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Imports we need in rnn.module.js\n",
    "*/\n",
    "import {round,flatten,exp,shape,transpose,dotProduct,randn,uniform,full,zeros} from './src/util.module.js';\n",
    "import {mean,reshape,argmax,normalize,identity,meanAndStandardDeviation} from './src/util.module.js';\n",
    "import {matrixSum1d,matrixSum2d,matrixSubtract1d,matrixSubtract2d,matrixMultiply1d,matrixMultiply2d} from './src/util.module.js';\n",
    "import {head,tail,parseCsv,IRIS_CLASS_MAP,IrisRowHandler,shuffle,split,batches} from './src/data.module.js';\n",
    "import {accuracy,Sigmoid,MSE,ReLU,Linear,Embedding,Learner} from './src/nn.module.js';\n",
    "import {BinaryCrossEntropyLoss,CrossEntropyLoss} from './src/nn.module.js';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "available-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Imports we need for testing\n",
    "import {testEq} from './src/testutil.module.js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contained-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Convert a 1d array of numbers (sequence of word indices) to a 2d array of shape [sequenceLength+1, nums.length/sequenceLength].\n",
    "This makes it easy to iterate over the 1st dimention of \"data\" to access a chunk of \"nums\", one timestep at a time.\n",
    "*/\n",
    "function toData(nums,sequenceLength) {\n",
    "    const data=full(sequenceLength+1).map(e=>[]);\n",
    "    const iMax=nums.length-sequenceLength;\n",
    "    for (let i=0; i<iMax; i+=sequenceLength) {\n",
    "        for (let j=0; j<sequenceLength+1; j++) {\n",
    "            data[j].push(nums[i+j]);\n",
    "        }\n",
    "    }\n",
    "    return data;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "altered-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEq([3,Math.floor(nums.length/2)],shape(toData(nums,2)));\n",
    "testEq([4,Math.floor(nums.length/3)],shape(toData(nums,3)));\n",
    "testEq([5,Math.floor(nums.length/4)],shape(toData(nums,4)));\n",
    "testEq([17,Math.floor(nums.length/16)],shape(toData(nums,16)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eligible-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "A layer that can wrap a Linear or Embedding so that it can be called multiple times during a forward pass.\n",
    "*/\n",
    "class MultiCallLayer {\n",
    "    constructor(layer) {\n",
    "        this.layer=layer;\n",
    "        this.xHistory=[];\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "    forward(x) {\n",
    "        this.xHistory.push(x);\n",
    "        return this.layer.forward(x);\n",
    "    }\n",
    "    _matrixSum2d(a,b) {\n",
    "        return (b == null) ? a : matrixSum2d(a,b);\n",
    "    }\n",
    "    _matrixSum1d(a,b) {\n",
    "        return (b == null) ? a : matrixSum1d(a,b);\n",
    "    }\n",
    "    backward(gradient) {\n",
    "        if (this.xHistory.length == 0) {\n",
    "            throw `this.xHistory is empty`;\n",
    "        }\n",
    "        this.x=this.xHistory.pop();\n",
    "        this.layer.backward(gradient);\n",
    "        this.weightsGradients=this._matrixSum2d(this.layer.weightsGradient,this.weightsGradients);\n",
    "        this.biasGradients=this._matrixSum1d(this.layer.biasGradient,this.biasGradients);\n",
    "        // Note: we're not keeping x gradients\n",
    "        return this.layer.xGradient;\n",
    "    }\n",
    "    update(lr) {\n",
    "        if (this.xHistory.length != 0) {\n",
    "            throw `forward has been called ${this.xHistory.length} times more than backward`;\n",
    "        }\n",
    "        this.layer.weightsGradient=this.weightsGradients;\n",
    "        this.layer.biasGradient=this.biasGradients;\n",
    "        this.layer.update(lr);\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-spoke",
   "metadata": {},
   "source": [
    "## TODO: test MultiCallLinear &uarr;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-acoustic",
   "metadata": {},
   "source": [
    "## First recurrent model - predict next word from previous 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        this.sequenceLength = sequenceLength || 3;\n",
    "        this.i_h = new MultiCallLayer(new Linear(vocab_sz, n_hidden));\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new Linear(n_hidden, vocab_sz);\n",
    "        this.non_linear = new ReLU();\n",
    "        this.oneHotLookup = normalize(identity(vocab_sz));\n",
    "    }\n",
    "    \n",
    "    toOneHot(x) {\n",
    "        return x.map(e=>this.oneHotLookup[e]);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<this.sequenceLength; i++) {\n",
    "            h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        let g=this.h_o.backward(gradients);\n",
    "        for (let i=this.sequenceLength; i>0; i--) {\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-sucking",
   "metadata": {},
   "source": [
    "`constructor`\n",
    "- normalizing the one-hot lookup seems to make training more stable and slightly more accurate\n",
    "\n",
    "`forward`\n",
    "- If `i_h` was an embedding we wouldn't need `toOneHot`\n",
    "- When we `matrixSum2d(i_h.forward(), h)` we put `h` on the right so that the initial value `0` is re-shaped to match the shape of the output of `i_h` (which is `[n_hidden,n_hidden]`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surface-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model_fn,data,dropLastBatch=false,shuffleBatch=true) {\n",
    "    console.log('Training model',model_fn);\n",
    "    let lossFn=new CrossEntropyLoss();\n",
    "    let model=new model_fn(vocab.length,28,data.length-1); // data.length-1 is sequence length\n",
    "    let yTrue=data[model.sequenceLength];\n",
    "    let lossValues=[];\n",
    "    let accuracyValues=[];\n",
    "    for (let epoch=0; epoch<15; epoch++) {\n",
    "        batches(data,64,dropLastBatch,shuffleBatch).forEach(batch => {\n",
    "            const xb=batch; // the model will look at only sequence length tokens\n",
    "            const yb=batch[model.sequenceLength];\n",
    "            let preds=model.forward(xb);\n",
    "            let lossValue=lossFn.forward(preds,yb);\n",
    "            lossValues.push(lossValue);\n",
    "            accuracyValues.push(accuracy(preds,yb));\n",
    "            model.backward(lossFn.backward());\n",
    "            model.update(6e-2);\n",
    "        });\n",
    "        console.log('epoch',epoch,new Date(),'train loss',round(mean(lossValues),3),\n",
    "                    'accuracy',round(mean(accuracyValues),3));\n",
    "        if (model.reset) {\n",
    "            model.reset();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-documentation",
   "metadata": {},
   "source": [
    "Train a model to predict the next word after 3 input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excellent-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2]\n",
      "epoch 0 2021-05-14T13:16:34.081Z train loss 2.518 accuracy 0.403\n",
      "epoch 1 2021-05-14T13:16:35.155Z train loss 2.121 accuracy 0.461\n",
      "epoch 2 2021-05-14T13:16:36.235Z train loss 1.938 accuracy 0.488\n",
      "epoch 3 2021-05-14T13:16:37.440Z train loss 1.833 accuracy 0.504\n",
      "epoch 4 2021-05-14T13:16:38.424Z train loss 1.765 accuracy 0.513\n",
      "epoch 5 2021-05-14T13:16:39.402Z train loss 1.713 accuracy 0.52\n",
      "epoch 6 2021-05-14T13:16:40.479Z train loss 1.673 accuracy 0.525\n",
      "epoch 7 2021-05-14T13:16:45.092Z train loss 1.64 accuracy 0.528\n",
      "epoch 8 2021-05-14T13:16:50.879Z train loss 1.612 accuracy 0.531\n",
      "epoch 9 2021-05-14T13:16:57.424Z train loss 1.589 accuracy 0.535\n",
      "epoch 10 2021-05-14T13:17:03.759Z train loss 1.57 accuracy 0.537\n",
      "epoch 11 2021-05-14T13:17:09.520Z train loss 1.553 accuracy 0.54\n",
      "epoch 12 2021-05-14T13:17:16.085Z train loss 1.538 accuracy 0.542\n",
      "epoch 13 2021-05-14T13:17:22.040Z train loss 1.526 accuracy 0.543\n",
      "epoch 14 2021-05-14T13:17:27.515Z train loss 1.515 accuracy 0.545\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2,toData(nums,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-output",
   "metadata": {},
   "source": [
    "Train a model to predict the next word after 4 input words &darr; to show this model and training approach are not just doing well because of the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gross-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2]\n",
      "epoch 0 2021-05-14T13:17:32.786Z train loss 2.862 accuracy 0.353\n",
      "epoch 1 2021-05-14T13:17:38.123Z train loss 2.38 accuracy 0.423\n",
      "epoch 2 2021-05-14T13:17:43.437Z train loss 2.149 accuracy 0.458\n",
      "epoch 3 2021-05-14T13:17:48.883Z train loss 2.005 accuracy 0.48\n",
      "epoch 4 2021-05-14T13:17:54.129Z train loss 1.909 accuracy 0.492\n",
      "epoch 5 2021-05-14T13:17:59.868Z train loss 1.84 accuracy 0.502\n",
      "epoch 6 2021-05-14T13:18:05.631Z train loss 1.792 accuracy 0.508\n",
      "epoch 7 2021-05-14T13:18:11.316Z train loss 1.756 accuracy 0.512\n",
      "epoch 8 2021-05-14T13:18:17.161Z train loss 1.723 accuracy 0.516\n",
      "epoch 9 2021-05-14T13:18:22.700Z train loss 1.696 accuracy 0.519\n",
      "epoch 10 2021-05-14T13:18:28.621Z train loss 1.671 accuracy 0.523\n",
      "epoch 11 2021-05-14T13:18:33.920Z train loss 1.654 accuracy 0.525\n",
      "epoch 12 2021-05-14T13:18:39.283Z train loss 1.637 accuracy 0.527\n",
      "epoch 13 2021-05-14T13:18:44.549Z train loss 1.619 accuracy 0.529\n",
      "epoch 14 2021-05-14T13:18:51.667Z train loss 1.605 accuracy 0.531\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2,toData(nums,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-blocking",
   "metadata": {},
   "source": [
    "### What if we normalize the signal/grads coming back from loss?\n",
    "\n",
    "`gradients` is used in a `dotProduct` to calculate weights and x gradients or each `Linear`-  might we get better training stability by normalizing the gradients we get back from the loss function?\n",
    "\n",
    "Adding the following to the top of `backward` shows us the mean and standard deviation of `gradients` and then normalizes them.\n",
    "\n",
    "```\n",
    "        let stats=meanAndStandardDeviation(flatten(gradients))\n",
    "        console.log('grads from loss function [mean,std]',stats);\n",
    "        gradients=normalize(gradients);\n",
    "```\n",
    "\n",
    "This increases the standard deviation from ~0.2 to 1.0 so we reduce `lr` by about 5x - which trains pretty much like it did before (o:\n",
    "\n",
    "Maybe this will help more when we try longer sequence lengths?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-astrology",
   "metadata": {},
   "source": [
    "## LMModel2 with an `Embedding`\n",
    "\n",
    "Using an embedding for the input to hidden layer makes training ~25% faster but ... if we one-hot encoded x, rather than call `toOneHot` in `LMModel2#forward`, we might see less of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "known-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2_2 extends LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        super(vocab_sz, n_hidden, sequenceLength);\n",
    "        const embedding=new Embedding(vocab_sz, n_hidden);\n",
    "        embedding.weights=uniform(vocab_sz, n_hidden,-2,2);\n",
    "        this.i_h = new MultiCallLayer(embedding);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<3; i++) {\n",
    "            /* Use this chunk (and comment the line below) to see stats of i_h output\n",
    "            const _h=this.i_h.forward(x[i]);\n",
    "            console.log(meanAndStandardDeviation(flatten(_h)));\n",
    "            h = matrixSum2d(_h, h);\n",
    "            */\n",
    "            h = matrixSum2d(this.i_h.forward(x[i]), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "secondary-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2_2 extends LMModel2]\n",
      "epoch 0 2021-05-14T13:18:57.011Z train loss 2.75 accuracy 0.346\n",
      "epoch 1 2021-05-14T13:19:01.124Z train loss 2.357 accuracy 0.42\n",
      "epoch 2 2021-05-14T13:19:05.312Z train loss 2.16 accuracy 0.455\n",
      "epoch 3 2021-05-14T13:19:09.688Z train loss 2.039 accuracy 0.477\n",
      "epoch 4 2021-05-14T13:19:14.197Z train loss 1.955 accuracy 0.491\n",
      "epoch 5 2021-05-14T13:19:18.204Z train loss 1.891 accuracy 0.5\n",
      "epoch 6 2021-05-14T13:19:22.241Z train loss 1.839 accuracy 0.508\n",
      "epoch 7 2021-05-14T13:19:26.389Z train loss 1.797 accuracy 0.513\n",
      "epoch 8 2021-05-14T13:19:30.540Z train loss 1.763 accuracy 0.518\n",
      "epoch 9 2021-05-14T13:19:34.702Z train loss 1.733 accuracy 0.521\n",
      "epoch 10 2021-05-14T13:19:38.878Z train loss 1.708 accuracy 0.523\n",
      "epoch 11 2021-05-14T13:19:43.409Z train loss 1.689 accuracy 0.526\n",
      "epoch 12 2021-05-14T13:19:48.096Z train loss 1.671 accuracy 0.528\n",
      "epoch 13 2021-05-14T13:19:52.406Z train loss 1.655 accuracy 0.53\n",
      "epoch 14 2021-05-14T13:19:56.794Z train loss 1.64 accuracy 0.532\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2_2,toData(nums,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cable",
   "metadata": {},
   "source": [
    "## Initializing the weights of the `Embedding`\n",
    "\n",
    "When training with large batch sizes, initializing the weights of the embedding seems to make a big difference to how the model trains - ended up using the unusual uniform `[-2,2)` init to get similar loss/accuracy.\n",
    "Using batch size of 64, we see less of a difference.\n",
    "\n",
    "`LMModel2`: Baseline (not using Embedding)\n",
    "```\n",
    "[ 0.00399811878414716, 1.2616704376269678 ]                  <- [mean, stdev] of i_h output first timestep\n",
    "[ -0.15787033906300632, 1.4662360727552772 ]                 <- [mean, stdev] of i_h output last timestep\n",
    "epoch 19 loss 1.7658807424786231 accuracy 0.5088483528450858\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using Kaiming init\n",
    "```\n",
    "[ -0.007255579503492108, 0.25066918887623985 ]\n",
    "[ -0.013472155034471406, 0.30197370989884775 ]\n",
    "epoch 19 loss 2.4118482003014603 accuracy 0.3294309828478083\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[0,1)` init\n",
    "```\n",
    "[ 0.5006462868294974, 0.2901404931519155 ]\n",
    "[ 0.40933492469236854, 0.32056279613160615 ]\n",
    "epoch 19 loss 2.29436828263747 accuracy 0.3476722025592159\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-1,1)` init\n",
    "```\n",
    "[ -0.00029163015159714393, 0.5849167220165731 ]\n",
    "[ -0.01253279522022337, 0.6122285187374188 ]\n",
    "epoch 19 loss 2.0740886848722444 accuracy 0.44949632453035665\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-2,2)` init\n",
    "```\n",
    "[ -0.02280506165625058, 1.1528261720333792 ]\n",
    "[ -0.04128903245762719, 1.1474591470160227 ]\n",
    "epoch 19 loss 1.8707648806156076 accuracy 0.5031309556221073\n",
    "```\n",
    "\n",
    "Xavier init would use `0.3216` (`Math.sqrt(6/(vocab.length+28))`) for uniform init."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-camping",
   "metadata": {},
   "source": [
    "# Maintaining the State of an RNN\n",
    "\n",
    "Organise data so the model sees contiguous text over subsequent batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-gravity",
   "metadata": {},
   "source": [
    "```\n",
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "printable-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "function groupChunks(ds,bs=64) {\n",
    "    const m = Math.floor(ds[0].length/bs);\n",
    "    const newDs = [...Array(ds.length).keys()].map(i=>[]);\n",
    "    for (let i=0; i<m; i++) {\n",
    "        for (let j=0; j<bs; j++) {\n",
    "            for (let k=0; k<ds.length; k++) {\n",
    "                newDs[k].push(ds[k][i + m*j]);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return newDs;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "together-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4, 3648 ]\n",
      "0 0 x one . two y .\n",
      "0 1 x sixty six . y sixty\n",
      "0 2 x hundred eighteen . y one\n",
      "0 3 x three . one y hundred\n",
      "0 4 x eighty eight . y one\n",
      "1 0 x . three . y four\n",
      "1 1 x sixty seven . y sixty\n",
      "1 2 x one hundred nineteen y .\n",
      "1 3 x hundred fifty four y .\n",
      "1 4 x one hundred eighty y nine\n",
      "2 0 x four . five y .\n",
      "2 1 x sixty eight . y sixty\n",
      "2 2 x . one hundred y twenty\n",
      "2 3 x . one hundred y fifty\n",
      "2 4 x nine . one y hundred\n"
     ]
    }
   ],
   "source": [
    "let _dataTemp = toData(nums,3);\n",
    "let _data=groupChunks(_dataTemp);\n",
    "\n",
    "console.log(shape(_data));\n",
    "let _batches=batches(_data,64,true,false);\n",
    "for (let b=0; b<3; b++) {\n",
    "    let _batch=_batches[b];\n",
    "    [...Array(5).keys()].forEach(i => {\n",
    "        console.log(b,i,'x',vocab[_batch[0][i]],vocab[_batch[1][i]],vocab[_batch[2][i]],'y',vocab[_batch[3][i]]);\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-cameroon",
   "metadata": {},
   "source": [
    "here's how we could move the initialization of `h` out of `forward` &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "turned-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3 extends LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        super(vocab_sz, n_hidden, sequenceLength);\n",
    "        this.h=0;\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        for (let i=0; i<3; i++) {\n",
    "            this.h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), this.h);\n",
    "            this.h = this.non_linear.forward(this.h_h.forward(this.h));\n",
    "        }\n",
    "        return this.h_o.forward(this.h);\n",
    "    }\n",
    "        \n",
    "    reset() {\n",
    "        this.h=0;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-thing",
   "metadata": {},
   "source": [
    "without organising the data so the model sees contiguous text over subsequent batches, we loose a little accuracy (~2%) but i thought we'd loose more. Maybe if we were looking at validation accuracy we'd see more of a difference.\n",
    "\n",
    "organising the data with `groupChunks` improves ~2% over our fist model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concerned-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel3 extends LMModel2]\n",
      "epoch 0 2021-05-14T13:20:03.222Z train loss 2.955 accuracy 0.331\n",
      "epoch 1 2021-05-14T13:20:09.458Z train loss 2.626 accuracy 0.377\n",
      "epoch 2 2021-05-14T13:20:15.325Z train loss 2.423 accuracy 0.404\n",
      "epoch 3 2021-05-14T13:20:21.361Z train loss 2.237 accuracy 0.431\n",
      "epoch 4 2021-05-14T13:20:26.932Z train loss 2.09 accuracy 0.451\n",
      "epoch 5 2021-05-14T13:20:32.565Z train loss 1.983 accuracy 0.467\n",
      "epoch 6 2021-05-14T13:20:38.350Z train loss 1.902 accuracy 0.48\n",
      "epoch 7 2021-05-14T13:20:43.982Z train loss 1.836 accuracy 0.49\n",
      "epoch 8 2021-05-14T13:20:49.551Z train loss 1.782 accuracy 0.498\n",
      "epoch 9 2021-05-14T13:20:55.204Z train loss 1.736 accuracy 0.505\n",
      "epoch 10 2021-05-14T13:21:00.902Z train loss 1.698 accuracy 0.512\n",
      "epoch 11 2021-05-14T13:21:06.548Z train loss 1.666 accuracy 0.517\n",
      "epoch 12 2021-05-14T13:21:12.463Z train loss 1.639 accuracy 0.522\n",
      "epoch 13 2021-05-14T13:21:18.671Z train loss 1.614 accuracy 0.525\n",
      "epoch 14 2021-05-14T13:21:24.640Z train loss 1.593 accuracy 0.529\n"
     ]
    }
   ],
   "source": [
    "train(LMModel3,groupChunks(toData(nums,3)),true,false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-turkey",
   "metadata": {},
   "source": [
    "# Creating More Signal\n",
    "\n",
    "## This one is a work in progress\n",
    "\n",
    "I can't find a way to train this model to anything like the accuracy of the previous models - loss reduces for a few epochs, but then starts increasing and usually goes to infinity unless we stop training early.\n",
    "\n",
    "Here's some things I've tried;\n",
    "- normalizing gradients going in to `this.h_o.backward`\n",
    "    - seems strange that the model can train at all when we do this - it must be changing the sign of some gradients when we center to mean 0?\n",
    "- reset `h` at the start of forward\n",
    "    - with and without shuffling data in batches\n",
    "- different batch sizes\n",
    "    - up to putting all of the data into a single batch\n",
    "- different learning rates\n",
    "    - even `lr`s that look too slow to get anywhere near good accuracy can cause loss -> infinity\n",
    "- going backward over fewer timesteps than forward\n",
    "- different sequence lengths: 3, 4, 8, 16\n",
    "    - shorter sequences train faster ...\n",
    "- different model size (`n_hidden`): 28, 32, 48, 52, 64\n",
    "    - smaller models train faster ...\n",
    "- swapping `i_h` `Embedding` for `Linear`\n",
    "\n",
    "I thought that maybe this model might just not train with plain SDG? but ...\n",
    "\n",
    "```\n",
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter, \n",
    "                opt_func=partial(SGD,mom=0.0, wd=0.0, decouple_wd=False),\n",
    "                moms=(0,0,0),\n",
    "                wd=0)\n",
    "learn.fit(15, 3e-3)\n",
    "```\n",
    "\n",
    "| epoch | train_loss | valid_loss | accuracy | time  |\n",
    "|-------|------------|------------|----------|-------|\n",
    "| 0     | 3.349324   | 3.404294   | 0.087321 | 00:01 |\n",
    "| 1     | 3.264445   | 3.3102     | 0.157145 | 00:01 |\n",
    "| 2     | 3.160242   | 3.204797   | 0.226725 | 00:01 |\n",
    "| 3     | 3.034063   | 3.08055    | 0.234945 | 00:01 |\n",
    "| 4     | 2.889665   | 2.945293   | 0.308105 | 00:01 |\n",
    "| 5     | 2.744815   | 2.818976   | 0.310954 | 00:01 |\n",
    "| 6     | 2.608936   | 2.700353   | 0.337158 | 00:01 |\n",
    "| 7     | 2.480137   | 2.585766   | 0.367106 | 00:01 |\n",
    "| 8     | 2.359796   | 2.476585   | 0.390544 | 00:01 |\n",
    "| 9     | 2.250405   | 2.370503   | 0.418294 | 00:01 |\n",
    "| 10    | 2.151468   | 2.266698   | 0.434326 | 00:01 |\n",
    "| 11    | 2.062078   | 2.170193   | 0.440999 | 00:01 |\n",
    "| 12    | 1.982661   | 2.08777    | 0.450602 | 00:01 |\n",
    "| 13    | 1.914207   | 2.023855   | 0.455648 | 00:01 |\n",
    "| 14    | 1.856968   | 1.977419   | 0.458089 | 00:01 |\n",
    "\n",
    "Must be a mistake in here somewhere ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "covered-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4 {\n",
    "    constructor(vocab_sz, n_hidden) {\n",
    "        this.useEmbedding=true;\n",
    "        if (this.useEmbedding) {\n",
    "            const embedding=new Embedding(vocab_sz, n_hidden);\n",
    "            embedding.weights=uniform(vocab_sz, n_hidden,-2,2);\n",
    "            this.i_h = new MultiCallLayer(embedding);\n",
    "        } else {\n",
    "            this.i_h = new MultiCallLayer(new Linear(vocab_sz, n_hidden));\n",
    "        }\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new MultiCallLayer(new Linear(n_hidden, vocab_sz));\n",
    "        this.non_linear = new ReLU();\n",
    "        this.oneHotLookup = normalize(identity(vocab_sz));\n",
    "        this.h=0;\n",
    "    }\n",
    "    \n",
    "    toOneHot(x) {\n",
    "        return x.map(e=>this.oneHotLookup[e]);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        const result = [];\n",
    "        for (let i=0; i<x.length-1; i++) {\n",
    "            if (this.useEmbedding) {\n",
    "                this.h = matrixSum2d(this.i_h.forward(x[i]), this.h);\n",
    "            } else {\n",
    "                this.h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), this.h);\n",
    "            }\n",
    "            this.h = this.non_linear.forward(this.h_h.forward(this.h));\n",
    "            result.push(this.h_o.forward(this.h));\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        for (let i=gradients.length-1; i>=0; i--) {\n",
    "            let g=this.h_o.backward(gradients[i]);\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "    \n",
    "    reset() {\n",
    "        this.h=0;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "injured-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten {\n",
    "    forward(x) {\n",
    "        this.originalShape=shape(x);\n",
    "        return [].concat(...x);\n",
    "    }\n",
    "    backward(x) {\n",
    "        const result=[];\n",
    "        for (let i=0; i<this.originalShape[0]; i++) {\n",
    "            const startFrom=i*this.originalShape[1];\n",
    "            result.push(x.slice(startFrom,startFrom+this.originalShape[1]));\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-addiction",
   "metadata": {},
   "source": [
    "The following test shows that we can use the same flatten for multiple arrays, as long as the first 2 dimentions of the arrays are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optional-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "let a=[],b=[];\n",
    "for (let i=0; i<3; i++) {\n",
    "    a.push([]); b.push([]);\n",
    "    for (let j=0; j<2; j++) {\n",
    "        a[a.length-1].push(`${i}.${j}`);\n",
    "        let _b=[]; b[b.length-1].push(_b);\n",
    "        for (let k=0; k<4; k++) {\n",
    "            _b.push(`${i}.${j}.${k}`);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "let _a=JSON.stringify(a),_b=JSON.stringify(b); \n",
    "let flatten=new Flatten();\n",
    "let aFlat=flatten.forward(a);\n",
    "let bFlat=flatten.forward(b);\n",
    "testEq([6],shape(aFlat));\n",
    "testEq([6,4],shape(bFlat));\n",
    "let aUnflat=flatten.backward(aFlat);\n",
    "let bUnflat=flatten.backward(bFlat);\n",
    "testEq([3,2],shape(aUnflat));\n",
    "testEq([3,2,4],shape(bUnflat));\n",
    "testEq(_a,JSON.stringify(aUnflat));\n",
    "testEq(_b,JSON.stringify(bUnflat));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train2(model_fn,data,dropLastBatch=false,shuffleBatch=true) {\n",
    "    console.log('Training model',model_fn);\n",
    "    let lossFn=new CrossEntropyLoss();\n",
    "    let model=new model_fn(vocab.length,64,data.length-1); // data.length-1 is sequence length\n",
    "    let flatten=new Flatten();\n",
    "    let yTrue=data[model.sequenceLength];\n",
    "    let lossValues=[];\n",
    "    let accuracyValues=[];\n",
    "    for (let epoch=0; epoch<50; epoch++) {\n",
    "        batches(data,64,dropLastBatch,shuffleBatch).forEach(batch => {\n",
    "            const xb=batch; // the model will look at only sequence length tokens\n",
    "            const ybFlat=flatten.forward(batch.filter((e,i)=>i!=0));\n",
    "            let predsFlat=flatten.forward(model.forward(xb));\n",
    "            let lossValue=lossFn.forward(predsFlat,ybFlat);\n",
    "            if (lossValue==Infinity) {\n",
    "                throw 'lossValue==Infinity';\n",
    "            }\n",
    "            lossValues.push(lossValue);\n",
    "            accuracyValues.push(accuracy(predsFlat,ybFlat));\n",
    "            model.backward(flatten.backward(lossFn.backward()));\n",
    "            model.update(3e-3);\n",
    "        });\n",
    "        console.log('epoch',epoch,new Date(),'train loss',round(mean(lossValues),3),\n",
    "                    'accuracy',round(mean(accuracyValues),3));\n",
    "        if (model.reset) {\n",
    "            model.reset();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel4]\n",
      "epoch 0 2021-05-14T13:21:44.731Z train loss 5.105 accuracy 0.069\n",
      "epoch 1 2021-05-14T13:22:06.057Z train loss 4.734 accuracy 0.081\n",
      "epoch 2 2021-05-14T13:22:27.579Z train loss 4.554 accuracy 0.09\n",
      "epoch 3 2021-05-14T13:22:48.569Z train loss 4.443 accuracy 0.097\n",
      "epoch 4 2021-05-14T13:23:08.469Z train loss 4.366 accuracy 0.102\n",
      "epoch 5 2021-05-14T13:23:26.842Z train loss 4.311 accuracy 0.107\n",
      "epoch 6 2021-05-14T13:23:45.961Z train loss 4.269 accuracy 0.111\n",
      "epoch 7 2021-05-14T13:24:03.850Z train loss 4.237 accuracy 0.115\n",
      "epoch 8 2021-05-14T13:24:22.328Z train loss 4.214 accuracy 0.118\n",
      "epoch 9 2021-05-14T13:24:44.053Z train loss 4.195 accuracy 0.121\n",
      "epoch 10 2021-05-14T13:25:02.383Z train loss 4.182 accuracy 0.124\n",
      "epoch 11 2021-05-14T13:25:20.162Z train loss 4.175 accuracy 0.128\n",
      "epoch 12 2021-05-14T13:25:38.015Z train loss 4.173 accuracy 0.131\n",
      "epoch 13 2021-05-14T13:25:56.546Z train loss 4.176 accuracy 0.134\n",
      "epoch 14 2021-05-14T13:26:14.452Z train loss 4.183 accuracy 0.136\n",
      "epoch 15 2021-05-14T13:26:32.351Z train loss 4.196 accuracy 0.139\n",
      "epoch 16 2021-05-14T13:26:49.814Z train loss 4.225 accuracy 0.142\n",
      "epoch 17 2021-05-14T13:27:08.461Z train loss 4.296 accuracy 0.145\n",
      "epoch 18 2021-05-14T13:27:27.819Z train loss 4.291 accuracy 0.147\n",
      "epoch 19 2021-05-14T13:27:46.645Z train loss 4.3 accuracy 0.15\n",
      "epoch 20 2021-05-14T13:28:05.203Z train loss 4.325 accuracy 0.152\n",
      "epoch 21 2021-05-14T13:28:24.489Z train loss 4.375 accuracy 0.154\n"
     ]
    }
   ],
   "source": [
    "train2(LMModel4,groupChunks(toData(nums,16)),true,false);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
