{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "//default_exp rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-restaurant",
   "metadata": {},
   "source": [
    "# rnn\n",
    "\n",
    "> Implement some of the language models in https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-section",
   "metadata": {},
   "source": [
    "## Set-up data used in tests / demos\n",
    "    \n",
    "Using [human_numbers.tgz](https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz), I created an even smaller dataset with the following;\n",
    "\n",
    "```\n",
    "lines = []\n",
    "with open('data/human_numbers/train.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/valid.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/train_and_valid.txt','w') as f:\n",
    "    f.write(' . '.join([l.strip() for l in lines[:2000]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo\n"
     ]
    }
   ],
   "source": [
    "const text=require('fs').readFileSync('data/human_numbers/train_and_valid.txt').toString();\n",
    "text.substring(0,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broadband-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  'one',      '.',        'two',\n",
      "  'three',    'four',     'five',\n",
      "  'six',      'seven',    'eight',\n",
      "  'nine',     'ten',      'eleven',\n",
      "  'twelve',   'thirteen', 'fourteen',\n",
      "  'fifteen',  'sixteen',  'seventeen',\n",
      "  'eighteen', 'nineteen', 'twenty',\n",
      "  'thirty',   'forty',    'fifty',\n",
      "  'sixty',    'seventy',  'eighty',\n",
      "  'ninety',   'hundred',  'thousand'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const tokens=text.split(' ');\n",
    "const vocab = [...new Set(tokens)];\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  one: 0,\n",
      "  '.': 1,\n",
      "  two: 2,\n",
      "  three: 3,\n",
      "  four: 4,\n",
      "  five: 5,\n",
      "  six: 6,\n",
      "  seven: 7,\n",
      "  eight: 8,\n",
      "  nine: 9,\n",
      "  ten: 10,\n",
      "  eleven: 11,\n",
      "  twelve: 12,\n",
      "  thirteen: 13,\n",
      "  fourteen: 14,\n",
      "  fifteen: 15,\n",
      "  sixteen: 16,\n",
      "  seventeen: 17,\n",
      "  eighteen: 18,\n",
      "  nineteen: 19,\n",
      "  twenty: 20,\n",
      "  thirty: 21,\n",
      "  forty: 22,\n",
      "  fifty: 23,\n",
      "  sixty: 24,\n",
      "  seventy: 25,\n",
      "  eighty: 26,\n",
      "  ninety: 27,\n",
      "  hundred: 28,\n",
      "  thousand: 29\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const word2idx={};\n",
    "vocab.forEach((word,idx)=>word2idx[word]=idx);\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dense-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   0,  1,  2,  1,  3, 1,  4,  1,  5,  1,  6, 1,\n",
      "   7,  1,  8,  1,  9, 1, 10,  1, 11,  1, 12, 1,\n",
      "  13,  1, 14,  1, 15, 1, 16,  1, 17,  1, 18, 1,\n",
      "  19,  1, 20,  1, 20, 0,  1, 20,  2,  1, 20, 3,\n",
      "   1, 20,  4,  1, 20, 5,  1, 20,  6,  1, 20, 7,\n",
      "   1, 20,  8,  1, 20, 9,  1, 21,  1, 21,  0, 1,\n",
      "  21,  2,  1, 21,  3, 1, 21,  4,  1, 21,  5, 1,\n",
      "  21,  6,  1, 21,  7, 1, 21,  8,  1, 21,  9, 1,\n",
      "  22,  1, 22,  0,\n",
      "  ... 10921 more items\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const nums=tokens.map(e=>word2idx[e]);\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "const data=[[],[],[],[]];\n",
    "for (let i=0; i<nums.length-4; i+=3) {\n",
    "    data[0].push(nums[i]);\n",
    "    data[1].push(nums[i+1]);\n",
    "    data[2].push(nums[i+2]);\n",
    "    data[3].push(nums[i+3]);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostCommonToken [ '.', 1999 ] 0.18138099990926412\n"
     ]
    }
   ],
   "source": [
    "const tokenCounter={};\n",
    "vocab.forEach(token=>tokenCounter[token]=0);\n",
    "tokens.forEach(token=>tokenCounter[token]++);\n",
    "let mostCommonToken=[null,0];\n",
    "for (let key in tokenCounter) {\n",
    "    if (tokenCounter[key]>mostCommonToken[1]) {\n",
    "        mostCommonToken=[key,tokenCounter[key]];\n",
    "    }\n",
    "}\n",
    "console.log('mostCommonToken', mostCommonToken, mostCommonToken[1]/tokens.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-easter",
   "metadata": {},
   "source": [
    "&uarr; if we can get better than 0.18 accuracy, our model will be doing better than predicting the most common token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guided-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Imports we need in rnn.module.js\n",
    "*/\n",
    "import {round,flatten,exp,shape,transpose,dotProduct,randn,uniform,full,zeros} from './src/util.module.js';\n",
    "import {mean,reshape,argmax,normalize,identity,meanAndStandardDeviation} from './src/util.module.js';\n",
    "import {matrixSum1d,matrixSum2d,matrixSubtract1d,matrixSubtract2d,matrixMultiply1d,matrixMultiply2d} from './src/util.module.js';\n",
    "import {head,tail,parseCsv,IRIS_CLASS_MAP,IrisRowHandler,shuffle,split,batches} from './src/data.module.js';\n",
    "import {accuracy,Sigmoid,MSE,ReLU,Linear,Embedding,Learner} from './src/nn.module.js';\n",
    "import {BinaryCrossEntropyLoss,CrossEntropyLoss} from './src/nn.module.js';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eligible-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "A layer that can wrap a Linear or Embedding so that it can be called multiple times during a forward pass.\n",
    "*/\n",
    "class MultiCallLayer {\n",
    "    constructor(layer) {\n",
    "        this.layer=layer;\n",
    "        this.xHistory=[];\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "    forward(x) {\n",
    "        this.xHistory.push(x);\n",
    "        return this.layer.forward(x);\n",
    "    }\n",
    "    _matrixSum2d(a,b) {\n",
    "        return (b == null) ? a : matrixSum2d(a,b);\n",
    "    }\n",
    "    _matrixSum1d(a,b) {\n",
    "        return (b == null) ? a : matrixSum1d(a,b);\n",
    "    }\n",
    "    backward(gradient) {\n",
    "        if (this.xHistory.length == 0) {\n",
    "            throw `this.xHistory is empty`;\n",
    "        }\n",
    "        this.x=this.xHistory.pop();\n",
    "        this.layer.backward(gradient);\n",
    "        this.weightsGradients=this._matrixSum2d(this.layer.weightsGradient,this.weightsGradients);\n",
    "        this.biasGradients=this._matrixSum1d(this.layer.biasGradient,this.biasGradients);\n",
    "        // Note: we're not keeping x gradients\n",
    "        return this.layer.xGradient;\n",
    "    }\n",
    "    update(lr) {\n",
    "        if (this.xHistory.length != 0) {\n",
    "            throw `forward has been called ${this.xHistory.length} times more than backward`;\n",
    "        }\n",
    "        this.layer.weightsGradient=this.weightsGradients;\n",
    "        this.layer.biasGradient=this.biasGradients;\n",
    "        this.layer.update(lr);\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-spoke",
   "metadata": {},
   "source": [
    "## TODO: test MultiCallLinear &uarr;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-acoustic",
   "metadata": {},
   "source": [
    "## First recurrent model - predict next word from previous 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressing-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden) {\n",
    "        this.i_h = new MultiCallLayer(new Linear(vocab_sz, n_hidden));\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new Linear(n_hidden, vocab_sz);\n",
    "        this.non_linear = new ReLU();\n",
    "        this.oneHotLookup = normalize(identity(vocab_sz));\n",
    "    }\n",
    "    \n",
    "    toOneHot(x) {\n",
    "        return x.map(e=>this.oneHotLookup[e]);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<3; i++) {\n",
    "            h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        let g=this.h_o.backward(gradients);\n",
    "        for (let i=2; i>=0; i--) {\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-sucking",
   "metadata": {},
   "source": [
    "`constructor`\n",
    "- normalizing the one-hot lookup seems to make training more stable and slightly more accurate\n",
    "\n",
    "`forward`\n",
    "- If `i_h` was an embedding we wouldn't need `toOneHot`\n",
    "- When we `matrixSum2d(i_h.forward(), h)` we put `h` on the right so that the initial value `0` is re-shaped to match the shape of the output of `i_h` (which is `[n_hidden,n_hidden]`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "excellent-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 2021-04-07T14:33:56.298Z loss 6.85558588702092 accuracy 0.03947726653961339\n",
      "epoch 1 2021-04-07T14:33:57.221Z loss 3.706164158094287 accuracy 0.19330247753879662\n",
      "epoch 2 2021-04-07T14:33:58.120Z loss 2.9473966477678295 accuracy 0.2627280152463926\n",
      "epoch 3 2021-04-07T14:33:59.045Z loss 2.6045549711621745 accuracy 0.3797985298121427\n",
      "epoch 4 2021-04-07T14:33:59.919Z loss 2.4233704445707223 accuracy 0.4173699972774299\n",
      "epoch 5 2021-04-07T14:34:00.878Z loss 2.3182864822502993 accuracy 0.4372447590525456\n",
      "epoch 6 2021-04-07T14:34:01.885Z loss 2.2319829917692404 accuracy 0.44949632453035665\n",
      "epoch 7 2021-04-07T14:34:02.783Z loss 2.1619745274058455 accuracy 0.4628369180506398\n",
      "epoch 8 2021-04-07T14:34:03.714Z loss 2.0989276292689296 accuracy 0.47753879662401305\n",
      "epoch 9 2021-04-07T14:34:04.728Z loss 2.0444898877931754 accuracy 0.49278518921862235\n",
      "epoch 10 2021-04-07T14:34:05.715Z loss 1.9953668202121395 accuracy 0.49686904437789275\n",
      "epoch 11 2021-04-07T14:34:07.038Z loss 1.9510350818508193 accuracy 0.5050367546964334\n",
      "epoch 12 2021-04-07T14:34:08.128Z loss 1.9103474345533127 accuracy 0.5036754696433433\n",
      "epoch 13 2021-04-07T14:34:09.101Z loss 1.8715594022548498 accuracy 0.5096651238769399\n",
      "epoch 14 2021-04-07T14:34:10.153Z loss 1.8357679166106764 accuracy 0.5175605771848625\n",
      "epoch 15 2021-04-07T14:34:11.169Z loss 1.8022986937477876 accuracy 0.5200108902804247\n",
      "epoch 16 2021-04-07T14:34:12.125Z loss 1.7715894741841183 accuracy 0.523005717397223\n",
      "epoch 17 2021-04-07T14:34:13.069Z loss 1.743256155236785 accuracy 0.5287231146202015\n",
      "epoch 18 2021-04-07T14:34:14.040Z loss 1.7177224851829647 accuracy 0.5314456847263817\n",
      "epoch 19 2021-04-07T14:34:15.180Z loss 1.6940315143658626 accuracy 0.533895997821944\n"
     ]
    }
   ],
   "source": [
    "let lossFn=new CrossEntropyLoss();\n",
    "let model=new LMModel2(vocab.length,28);\n",
    "let yTrue=data[3];\n",
    "for (let epoch=0; epoch<20; epoch++) {\n",
    "    let yPred=model.forward(data);\n",
    "    let lossValue=lossFn.forward(yPred,yTrue);\n",
    "    console.log('epoch',epoch,new Date(),'loss',lossValue,'accuracy',accuracy(yPred,yTrue));\n",
    "    model.backward(lossFn.backward());\n",
    "    model.update(3e-1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-pension",
   "metadata": {},
   "source": [
    "```\n",
    "let lossFn=new CrossEntropyLoss();\n",
    "let model=new LMModel2(vocab.length,28);\n",
    "let yTrue=data[3];\n",
    "for (let epoch=0; epoch<10; epoch++) {\n",
    "    let yPred=model.forward(data);\n",
    "    let lossValue=lossFn.forward(yPred,yTrue);\n",
    "    console.log('epoch',epoch,'loss',lossValue,'accuracy',accuracy(yPred,yTrue));\n",
    "    model.backward(lossFn.backward());\n",
    "    model.update(3e-1);\n",
    "}\n",
    "epoch 0 loss 3.2278166493704066 accuracy 0.07296487884563027\n",
    "...\n",
    "epoch 9 loss 2.3760766295703397 accuracy 0.43642798802069155\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-blocking",
   "metadata": {},
   "source": [
    "### What if we normalize the signal/grads coming back from loss?\n",
    "\n",
    "`gradients` is used in a `dotProduct` to calculate weights and x gradients or each `Linear`-  might we get better training stability by normalizing the gradients we get back from the loss function?\n",
    "\n",
    "Adding the following to the top of `backward` shows us the mean and standard deviation of `gradients` and then normalizes them.\n",
    "\n",
    "```\n",
    "        let stats=meanAndStandardDeviation(flatten(gradients))\n",
    "        console.log('grads from loss function [mean,std]',stats);\n",
    "        gradients=normalize(gradients);\n",
    "```\n",
    "\n",
    "This increases the standard deviation from ~0.2 to 1.0 so we reduce `lr` by about 5x - which trains pretty much like it did before (o:\n",
    "\n",
    "Maybe this will help more when we try longer sequence lengths?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-astrology",
   "metadata": {},
   "source": [
    "## LMModel2 with an `Embedding`\n",
    "\n",
    "Using an embedding for the input to hidden layer makes training ~25% faster but ... if we one-hot encoded x, rather than call `toOneHot` in `LMModel2#forward`, we might see less of a difference.\n",
    "\n",
    "Initializing the weights of the embedding seems to make a big difference to how the model trains - ended up using the unusual uniform `[-2,2)` init to get similar loss/accuracy.\n",
    "\n",
    "`LMModel2`: Baseline (not using Embedding)\n",
    "```\n",
    "[ 0.00399811878414716, 1.2616704376269678 ]                  <- [mean, stdev] of i_h output first timestep\n",
    "[ -0.15787033906300632, 1.4662360727552772 ]                 <- [mean, stdev] of i_h output last timestep\n",
    "epoch 19 loss 1.7658807424786231 accuracy 0.5088483528450858\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using Kaiming init\n",
    "```\n",
    "[ -0.007255579503492108, 0.25066918887623985 ]\n",
    "[ -0.013472155034471406, 0.30197370989884775 ]\n",
    "epoch 19 loss 2.4118482003014603 accuracy 0.3294309828478083\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[0,1)` init\n",
    "```\n",
    "[ 0.5006462868294974, 0.2901404931519155 ]\n",
    "[ 0.40933492469236854, 0.32056279613160615 ]\n",
    "epoch 19 loss 2.29436828263747 accuracy 0.3476722025592159\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-1,1)` init\n",
    "```\n",
    "[ -0.00029163015159714393, 0.5849167220165731 ]\n",
    "[ -0.01253279522022337, 0.6122285187374188 ]\n",
    "epoch 19 loss 2.0740886848722444 accuracy 0.44949632453035665\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-2,2)` init\n",
    "```\n",
    "[ -0.02280506165625058, 1.1528261720333792 ]\n",
    "[ -0.04128903245762719, 1.1474591470160227 ]\n",
    "epoch 19 loss 1.8707648806156076 accuracy 0.5031309556221073\n",
    "```\n",
    "\n",
    "Xavier init would use `0.3216` (`Math.sqrt(6/(vocab.length+28))`) for uniform init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "known-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2_2 {\n",
    "    constructor(vocab_sz, n_hidden) {\n",
    "        const embedding=new Embedding(vocab_sz, n_hidden);\n",
    "        embedding.weights=uniform(vocab_sz, n_hidden,-2,2);\n",
    "        this.i_h = new MultiCallLayer(embedding);\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new Linear(n_hidden, vocab_sz);\n",
    "        this.non_linear = new ReLU();\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<3; i++) {\n",
    "            /* Use this chunk (and comment the line below) to see stats of i_h output\n",
    "            const _h=this.i_h.forward(x[i]);\n",
    "            console.log(meanAndStandardDeviation(flatten(_h)));\n",
    "            h = matrixSum2d(_h, h);\n",
    "            */\n",
    "            h = matrixSum2d(this.i_h.forward(x[i]), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        let g=this.h_o.backward(gradients);\n",
    "        for (let i=2; i>=0; i--) {\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "secondary-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 2021-04-07T14:37:00.302Z loss 7.313158874221775 accuracy 0.009801252382248844\n",
      "epoch 1 2021-04-07T14:37:01.156Z loss 4.201322620685817 accuracy 0.04029403757146747\n",
      "epoch 2 2021-04-07T14:37:02.117Z loss 3.5045546958712617 accuracy 0.08657772937653145\n",
      "epoch 3 2021-04-07T14:37:03.115Z loss 3.1463271031454774 accuracy 0.14456847263817044\n",
      "epoch 4 2021-04-07T14:37:04.031Z loss 2.9013633222434336 accuracy 0.23985842635447863\n",
      "epoch 5 2021-04-07T14:37:04.960Z loss 2.7227818060725757 accuracy 0.3087394500408385\n",
      "epoch 6 2021-04-07T14:37:05.822Z loss 2.5738498626440065 accuracy 0.36591342227062346\n",
      "epoch 7 2021-04-07T14:37:06.681Z loss 2.452707705533566 accuracy 0.39912877756602233\n",
      "epoch 8 2021-04-07T14:37:07.686Z loss 2.358470421570021 accuracy 0.4244486795534985\n",
      "epoch 9 2021-04-07T14:37:08.621Z loss 2.284588957613364 accuracy 0.44432344132861423\n",
      "epoch 10 2021-04-07T14:37:09.466Z loss 2.2242727992122493 accuracy 0.46202014701878574\n",
      "epoch 11 2021-04-07T14:37:10.321Z loss 2.1725631749838397 accuracy 0.47263817043288864\n",
      "epoch 12 2021-04-07T14:37:11.184Z loss 2.121665580419366 accuracy 0.476177511570923\n",
      "epoch 13 2021-04-07T14:37:12.041Z loss 2.0771590568708698 accuracy 0.4813503947726654\n",
      "epoch 14 2021-04-07T14:37:12.942Z loss 2.0382730127810333 accuracy 0.4859787639531718\n",
      "epoch 15 2021-04-07T14:37:13.824Z loss 1.9993790264589422 accuracy 0.4941464742717125\n",
      "epoch 16 2021-04-07T14:37:14.694Z loss 1.9663406151918956 accuracy 0.49686904437789275\n",
      "epoch 17 2021-04-07T14:37:15.645Z loss 1.9365566747089078 accuracy 0.4976858154097468\n",
      "epoch 18 2021-04-07T14:37:16.603Z loss 1.9105666734745692 accuracy 0.4990471004628369\n",
      "epoch 19 2021-04-07T14:37:17.529Z loss 1.8870242524320335 accuracy 0.5014974135583992\n"
     ]
    }
   ],
   "source": [
    "let lossFn=new CrossEntropyLoss();\n",
    "let model=new LMModel2_2(vocab.length,28);\n",
    "let yTrue=data[3];\n",
    "for (let epoch=0; epoch<20; epoch++) {\n",
    "    let yPred=model.forward(data);\n",
    "    let lossValue=lossFn.forward(yPred,yTrue);\n",
    "    console.log('epoch',epoch,new Date(),'loss',lossValue,'accuracy',accuracy(yPred,yTrue));\n",
    "    model.backward(lossFn.backward());\n",
    "    model.update(3e-1);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
