{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "//default_exp rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-restaurant",
   "metadata": {},
   "source": [
    "# rnn\n",
    "\n",
    "> Implement some of the language models in https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-section",
   "metadata": {},
   "source": [
    "## Set-up data used in tests / demos\n",
    "    \n",
    "Using [human_numbers.tgz](https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz), I created an even smaller dataset with the following;\n",
    "\n",
    "```\n",
    "lines = []\n",
    "with open('data/human_numbers/train.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/valid.txt') as f: lines.extend(f.readlines())\n",
    "with open('data/human_numbers/train_and_valid.txt','w') as f:\n",
    "    f.write(' . '.join([l.strip() for l in lines[:2000]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo\n"
     ]
    }
   ],
   "source": [
    "const text=require('fs').readFileSync('data/human_numbers/train_and_valid.txt').toString();\n",
    "text.substring(0,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadband-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  'one',      '.',        'two',\n",
      "  'three',    'four',     'five',\n",
      "  'six',      'seven',    'eight',\n",
      "  'nine',     'ten',      'eleven',\n",
      "  'twelve',   'thirteen', 'fourteen',\n",
      "  'fifteen',  'sixteen',  'seventeen',\n",
      "  'eighteen', 'nineteen', 'twenty',\n",
      "  'thirty',   'forty',    'fifty',\n",
      "  'sixty',    'seventy',  'eighty',\n",
      "  'ninety',   'hundred',  'thousand'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const tokens=text.split(' ');\n",
    "const vocab = [...new Set(tokens)];\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  one: 0,\n",
      "  '.': 1,\n",
      "  two: 2,\n",
      "  three: 3,\n",
      "  four: 4,\n",
      "  five: 5,\n",
      "  six: 6,\n",
      "  seven: 7,\n",
      "  eight: 8,\n",
      "  nine: 9,\n",
      "  ten: 10,\n",
      "  eleven: 11,\n",
      "  twelve: 12,\n",
      "  thirteen: 13,\n",
      "  fourteen: 14,\n",
      "  fifteen: 15,\n",
      "  sixteen: 16,\n",
      "  seventeen: 17,\n",
      "  eighteen: 18,\n",
      "  nineteen: 19,\n",
      "  twenty: 20,\n",
      "  thirty: 21,\n",
      "  forty: 22,\n",
      "  fifty: 23,\n",
      "  sixty: 24,\n",
      "  seventy: 25,\n",
      "  eighty: 26,\n",
      "  ninety: 27,\n",
      "  hundred: 28,\n",
      "  thousand: 29\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const word2idx={};\n",
    "vocab.forEach((word,idx)=>word2idx[word]=idx);\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dense-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   0,  1,  2,  1,  3, 1,  4,  1,  5,  1,  6, 1,\n",
      "   7,  1,  8,  1,  9, 1, 10,  1, 11,  1, 12, 1,\n",
      "  13,  1, 14,  1, 15, 1, 16,  1, 17,  1, 18, 1,\n",
      "  19,  1, 20,  1, 20, 0,  1, 20,  2,  1, 20, 3,\n",
      "   1, 20,  4,  1, 20, 5,  1, 20,  6,  1, 20, 7,\n",
      "   1, 20,  8,  1, 20, 9,  1, 21,  1, 21,  0, 1,\n",
      "  21,  2,  1, 21,  3, 1, 21,  4,  1, 21,  5, 1,\n",
      "  21,  6,  1, 21,  7, 1, 21,  8,  1, 21,  9, 1,\n",
      "  22,  1, 22,  0,\n",
      "  ... 10921 more items\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const nums=tokens.map(e=>word2idx[e]);\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similar-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostCommonToken [ '.', 1999 ] 0.18138099990926412\n"
     ]
    }
   ],
   "source": [
    "const tokenCounter={};\n",
    "vocab.forEach(token=>tokenCounter[token]=0);\n",
    "tokens.forEach(token=>tokenCounter[token]++);\n",
    "let mostCommonToken=[null,0];\n",
    "for (let key in tokenCounter) {\n",
    "    if (tokenCounter[key]>mostCommonToken[1]) {\n",
    "        mostCommonToken=[key,tokenCounter[key]];\n",
    "    }\n",
    "}\n",
    "console.log('mostCommonToken', mostCommonToken, mostCommonToken[1]/tokens.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-easter",
   "metadata": {},
   "source": [
    "&uarr; if we can get better than 0.18 accuracy, our model will be doing better than predicting the most common token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "guided-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Imports we need in rnn.module.js\n",
    "*/\n",
    "import {round,flatten,exp,shape,transpose,dotProduct,randn,uniform,full,zeros} from './src/util.module.js';\n",
    "import {mean,reshape,argmax,normalize,identity,meanAndStandardDeviation} from './src/util.module.js';\n",
    "import {matrixSum1d,matrixSum2d,matrixSubtract1d,matrixSubtract2d,matrixMultiply1d,matrixMultiply2d} from './src/util.module.js';\n",
    "import {head,tail,parseCsv,IRIS_CLASS_MAP,IrisRowHandler,shuffle,split,batches} from './src/data.module.js';\n",
    "import {accuracy,Sigmoid,MSE,ReLU,Linear,Embedding,Learner} from './src/nn.module.js';\n",
    "import {BinaryCrossEntropyLoss,CrossEntropyLoss} from './src/nn.module.js';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "available-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Imports we need for testing\n",
    "import {testEq} from './src/testutil.module.js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contained-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Convert a 1d array of numbers (sequence of word indices) to a 2d array of shape [sequenceLength+1, nums.length/sequenceLength].\n",
    "This makes it easy to iterate over the 1st dimention of \"data\" to access a chunk of \"nums\", one timestep at a time.\n",
    "*/\n",
    "function toData(nums,sequenceLength) {\n",
    "    const data=full(sequenceLength+1).map(e=>[]);\n",
    "    const iMax=nums.length-sequenceLength;\n",
    "    for (let i=0; i<iMax; i+=sequenceLength) {\n",
    "        for (let j=0; j<sequenceLength+1; j++) {\n",
    "            data[j].push(nums[i+j]);\n",
    "        }\n",
    "    }\n",
    "    return data;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "altered-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEq([3,Math.floor(nums.length/2)],shape(toData(nums,2)));\n",
    "testEq([4,Math.floor(nums.length/3)],shape(toData(nums,3)));\n",
    "testEq([5,Math.floor(nums.length/4)],shape(toData(nums,4)));\n",
    "testEq([17,Math.floor(nums.length/16)],shape(toData(nums,16)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eligible-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "A layer that can wrap a Linear or Embedding so that it can be called multiple times during a forward pass.\n",
    "*/\n",
    "class MultiCallLayer {\n",
    "    constructor(layer) {\n",
    "        this.layer=layer;\n",
    "        this.xHistory=[];\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "    forward(x) {\n",
    "        this.xHistory.push(x);\n",
    "        return this.layer.forward(x);\n",
    "    }\n",
    "    _matrixSum2d(a,b) {\n",
    "        return (b == null) ? a : matrixSum2d(a,b);\n",
    "    }\n",
    "    _matrixSum1d(a,b) {\n",
    "        return (b == null) ? a : matrixSum1d(a,b);\n",
    "    }\n",
    "    backward(gradient) {\n",
    "        if (this.xHistory.length == 0) {\n",
    "            throw `this.xHistory is empty`;\n",
    "        }\n",
    "        this.x=this.xHistory.pop();\n",
    "        this.layer.backward(gradient);\n",
    "        this.weightsGradients=this._matrixSum2d(this.layer.weightsGradient,this.weightsGradients);\n",
    "        this.biasGradients=this._matrixSum1d(this.layer.biasGradient,this.biasGradients);\n",
    "        // Note: we're not keeping x gradients\n",
    "        return this.layer.xGradient;\n",
    "    }\n",
    "    update(lr) {\n",
    "        if (this.xHistory.length != 0) {\n",
    "            throw `forward has been called ${this.xHistory.length} times more than backward`;\n",
    "        }\n",
    "        this.layer.weightsGradient=this.weightsGradients;\n",
    "        this.layer.biasGradient=this.biasGradients;\n",
    "        this.layer.update(lr);\n",
    "        this.weightsGradients=null;\n",
    "        this.biasGradients=null;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-spoke",
   "metadata": {},
   "source": [
    "## TODO: test MultiCallLinear &uarr;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-acoustic",
   "metadata": {},
   "source": [
    "## First recurrent model - predict next word from previous 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        this.sequenceLength = sequenceLength || 3;\n",
    "        this.i_h = new MultiCallLayer(new Linear(vocab_sz, n_hidden));\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new Linear(n_hidden, vocab_sz);\n",
    "        this.non_linear = new ReLU();\n",
    "        this.oneHotLookup = normalize(identity(vocab_sz));\n",
    "    }\n",
    "    \n",
    "    toOneHot(x) {\n",
    "        return x.map(e=>this.oneHotLookup[e]);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<this.sequenceLength; i++) {\n",
    "            h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        let g=this.h_o.backward(gradients);\n",
    "        for (let i=this.sequenceLength; i>0; i--) {\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-sucking",
   "metadata": {},
   "source": [
    "`constructor`\n",
    "- normalizing the one-hot lookup seems to make training more stable and slightly more accurate\n",
    "\n",
    "`forward`\n",
    "- If `i_h` was an embedding we wouldn't need `toOneHot`\n",
    "- When we `matrixSum2d(i_h.forward(), h)` we put `h` on the right so that the initial value `0` is re-shaped to match the shape of the output of `i_h` (which is `[n_hidden,n_hidden]`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surface-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model_fn,data,dropLastBatch=false,shuffleBatch=true) {\n",
    "    console.log('Training model',model_fn);\n",
    "    let lossFn=new CrossEntropyLoss();\n",
    "    let model=new model_fn(vocab.length,28,data.length-1); // data.length-1 is sequence length\n",
    "    let yTrue=data[model.sequenceLength];\n",
    "    let lossValues=[];\n",
    "    let accuracyValues=[];\n",
    "    for (let epoch=0; epoch<15; epoch++) {\n",
    "        batches(data,64,dropLastBatch,shuffleBatch).forEach(batch => {\n",
    "            const xb=batch; // the model will look at only sequence length tokens\n",
    "            const yb=batch[model.sequenceLength];\n",
    "            let preds=model.forward(xb);\n",
    "            let lossValue=lossFn.forward(preds,yb);\n",
    "            lossValues.push(lossValue);\n",
    "            accuracyValues.push(accuracy(preds,yb));\n",
    "            model.backward(lossFn.backward());\n",
    "            model.update(6e-2);\n",
    "        });\n",
    "        console.log('epoch',epoch,new Date(),'train loss',round(mean(lossValues),3),\n",
    "                    'accuracy',round(mean(accuracyValues),3));\n",
    "        if (model.reset) {\n",
    "            model.reset();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-documentation",
   "metadata": {},
   "source": [
    "Train a model to predict the next word after 3 input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excellent-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2]\n",
      "epoch 0 2021-05-14T10:05:01.072Z train loss 2.608 accuracy 0.36\n",
      "epoch 1 2021-05-14T10:05:02.089Z train loss 2.26 accuracy 0.416\n",
      "epoch 2 2021-05-14T10:05:03.099Z train loss 2.07 accuracy 0.448\n",
      "epoch 3 2021-05-14T10:05:04.141Z train loss 1.951 accuracy 0.467\n",
      "epoch 4 2021-05-14T10:05:05.174Z train loss 1.867 accuracy 0.479\n",
      "epoch 5 2021-05-14T10:05:06.241Z train loss 1.808 accuracy 0.487\n",
      "epoch 6 2021-05-14T10:05:07.446Z train loss 1.761 accuracy 0.494\n",
      "epoch 7 2021-05-14T10:05:08.536Z train loss 1.723 accuracy 0.501\n",
      "epoch 8 2021-05-14T10:05:09.601Z train loss 1.692 accuracy 0.506\n",
      "epoch 9 2021-05-14T10:05:10.662Z train loss 1.664 accuracy 0.511\n",
      "epoch 10 2021-05-14T10:05:11.698Z train loss 1.64 accuracy 0.515\n",
      "epoch 11 2021-05-14T10:05:12.783Z train loss 1.62 accuracy 0.518\n",
      "epoch 12 2021-05-14T10:05:13.931Z train loss 1.604 accuracy 0.52\n",
      "epoch 13 2021-05-14T10:05:15.013Z train loss 1.59 accuracy 0.522\n",
      "epoch 14 2021-05-14T10:05:16.150Z train loss 1.577 accuracy 0.524\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2,toData(nums,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-output",
   "metadata": {},
   "source": [
    "Train a model to predict the next word after 4 input words &darr; to show this model and training approach are not just doing well because of the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gross-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2]\n",
      "epoch 0 2021-05-14T10:05:17.258Z train loss 2.864 accuracy 0.343\n",
      "epoch 1 2021-05-14T10:05:18.250Z train loss 2.381 accuracy 0.41\n",
      "epoch 2 2021-05-14T10:05:19.243Z train loss 2.158 accuracy 0.446\n",
      "epoch 3 2021-05-14T10:05:20.286Z train loss 2.022 accuracy 0.469\n",
      "epoch 4 2021-05-14T10:05:21.350Z train loss 1.936 accuracy 0.482\n",
      "epoch 5 2021-05-14T10:05:22.390Z train loss 1.865 accuracy 0.493\n",
      "epoch 6 2021-05-14T10:05:23.497Z train loss 1.81 accuracy 0.501\n",
      "epoch 7 2021-05-14T10:05:24.620Z train loss 1.767 accuracy 0.506\n",
      "epoch 8 2021-05-14T10:05:25.983Z train loss 1.728 accuracy 0.512\n",
      "epoch 9 2021-05-14T10:05:27.281Z train loss 1.699 accuracy 0.516\n",
      "epoch 10 2021-05-14T10:05:28.576Z train loss 1.674 accuracy 0.519\n",
      "epoch 11 2021-05-14T10:05:29.758Z train loss 1.654 accuracy 0.521\n",
      "epoch 12 2021-05-14T10:05:31.281Z train loss 1.636 accuracy 0.523\n",
      "epoch 13 2021-05-14T10:05:32.424Z train loss 1.621 accuracy 0.524\n",
      "epoch 14 2021-05-14T10:05:33.519Z train loss 1.607 accuracy 0.526\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2,toData(nums,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-blocking",
   "metadata": {},
   "source": [
    "### What if we normalize the signal/grads coming back from loss?\n",
    "\n",
    "`gradients` is used in a `dotProduct` to calculate weights and x gradients or each `Linear`-  might we get better training stability by normalizing the gradients we get back from the loss function?\n",
    "\n",
    "Adding the following to the top of `backward` shows us the mean and standard deviation of `gradients` and then normalizes them.\n",
    "\n",
    "```\n",
    "        let stats=meanAndStandardDeviation(flatten(gradients))\n",
    "        console.log('grads from loss function [mean,std]',stats);\n",
    "        gradients=normalize(gradients);\n",
    "```\n",
    "\n",
    "This increases the standard deviation from ~0.2 to 1.0 so we reduce `lr` by about 5x - which trains pretty much like it did before (o:\n",
    "\n",
    "Maybe this will help more when we try longer sequence lengths?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-astrology",
   "metadata": {},
   "source": [
    "## LMModel2 with an `Embedding`\n",
    "\n",
    "Using an embedding for the input to hidden layer makes training ~25% faster but ... if we one-hot encoded x, rather than call `toOneHot` in `LMModel2#forward`, we might see less of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "known-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2_2 extends LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        super(vocab_sz, n_hidden, sequenceLength);\n",
    "        const embedding=new Embedding(vocab_sz, n_hidden);\n",
    "        embedding.weights=uniform(vocab_sz, n_hidden,-2,2);\n",
    "        this.i_h = new MultiCallLayer(embedding);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        let h=0;\n",
    "        for (let i=0; i<3; i++) {\n",
    "            /* Use this chunk (and comment the line below) to see stats of i_h output\n",
    "            const _h=this.i_h.forward(x[i]);\n",
    "            console.log(meanAndStandardDeviation(flatten(_h)));\n",
    "            h = matrixSum2d(_h, h);\n",
    "            */\n",
    "            h = matrixSum2d(this.i_h.forward(x[i]), h);\n",
    "            h = this.non_linear.forward(this.h_h.forward(h));\n",
    "        }\n",
    "        return this.h_o.forward(h);\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "secondary-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel2_2 extends LMModel2]\n",
      "epoch 0 2021-05-14T10:05:34.617Z train loss 2.692 accuracy 0.359\n",
      "epoch 1 2021-05-14T10:05:35.638Z train loss 2.283 accuracy 0.441\n",
      "epoch 2 2021-05-14T10:05:36.578Z train loss 2.089 accuracy 0.474\n",
      "epoch 3 2021-05-14T10:05:37.703Z train loss 1.973 accuracy 0.491\n",
      "epoch 4 2021-05-14T10:05:38.660Z train loss 1.891 accuracy 0.502\n",
      "epoch 5 2021-05-14T10:05:39.588Z train loss 1.829 accuracy 0.51\n",
      "epoch 6 2021-05-14T10:05:40.551Z train loss 1.782 accuracy 0.516\n",
      "epoch 7 2021-05-14T10:05:41.590Z train loss 1.742 accuracy 0.521\n",
      "epoch 8 2021-05-14T10:05:42.558Z train loss 1.71 accuracy 0.525\n",
      "epoch 9 2021-05-14T10:05:43.483Z train loss 1.684 accuracy 0.528\n",
      "epoch 10 2021-05-14T10:05:44.372Z train loss 1.661 accuracy 0.531\n",
      "epoch 11 2021-05-14T10:05:45.370Z train loss 1.641 accuracy 0.534\n",
      "epoch 12 2021-05-14T10:05:46.473Z train loss 1.624 accuracy 0.536\n",
      "epoch 13 2021-05-14T10:05:47.405Z train loss 1.609 accuracy 0.538\n",
      "epoch 14 2021-05-14T10:05:48.333Z train loss 1.595 accuracy 0.539\n"
     ]
    }
   ],
   "source": [
    "train(LMModel2_2,toData(nums,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cable",
   "metadata": {},
   "source": [
    "## Initializing the weights of the `Embedding`\n",
    "\n",
    "When training with large batch sizes, initializing the weights of the embedding seems to make a big difference to how the model trains - ended up using the unusual uniform `[-2,2)` init to get similar loss/accuracy.\n",
    "Using batch size of 64, we see less of a difference.\n",
    "\n",
    "`LMModel2`: Baseline (not using Embedding)\n",
    "```\n",
    "[ 0.00399811878414716, 1.2616704376269678 ]                  <- [mean, stdev] of i_h output first timestep\n",
    "[ -0.15787033906300632, 1.4662360727552772 ]                 <- [mean, stdev] of i_h output last timestep\n",
    "epoch 19 loss 1.7658807424786231 accuracy 0.5088483528450858\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using Kaiming init\n",
    "```\n",
    "[ -0.007255579503492108, 0.25066918887623985 ]\n",
    "[ -0.013472155034471406, 0.30197370989884775 ]\n",
    "epoch 19 loss 2.4118482003014603 accuracy 0.3294309828478083\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[0,1)` init\n",
    "```\n",
    "[ 0.5006462868294974, 0.2901404931519155 ]\n",
    "[ 0.40933492469236854, 0.32056279613160615 ]\n",
    "epoch 19 loss 2.29436828263747 accuracy 0.3476722025592159\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-1,1)` init\n",
    "```\n",
    "[ -0.00029163015159714393, 0.5849167220165731 ]\n",
    "[ -0.01253279522022337, 0.6122285187374188 ]\n",
    "epoch 19 loss 2.0740886848722444 accuracy 0.44949632453035665\n",
    "```\n",
    "\n",
    "`LMModel2_2`: Embedding using uniform `[-2,2)` init\n",
    "```\n",
    "[ -0.02280506165625058, 1.1528261720333792 ]\n",
    "[ -0.04128903245762719, 1.1474591470160227 ]\n",
    "epoch 19 loss 1.8707648806156076 accuracy 0.5031309556221073\n",
    "```\n",
    "\n",
    "Xavier init would use `0.3216` (`Math.sqrt(6/(vocab.length+28))`) for uniform init."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-camping",
   "metadata": {},
   "source": [
    "# Maintaining the State of an RNN\n",
    "\n",
    "Organise data so the model sees contiguous text over subsequent batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-gravity",
   "metadata": {},
   "source": [
    "```\n",
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "printable-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "function groupChunks(ds,bs=64) {\n",
    "    const m = Math.floor(ds[0].length/bs);\n",
    "    const newDs = [...Array(ds.length).keys()].map(i=>[]);\n",
    "    for (let i=0; i<m; i++) {\n",
    "        for (let j=0; j<bs; j++) {\n",
    "            for (let k=0; k<ds.length; k++) {\n",
    "                newDs[k].push(ds[k][i + m*j]);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return newDs;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "together-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4, 3648 ]\n",
      "0 0 x one . two y .\n",
      "0 1 x sixty six . y sixty\n",
      "0 2 x hundred eighteen . y one\n",
      "0 3 x three . one y hundred\n",
      "0 4 x eighty eight . y one\n",
      "1 0 x . three . y four\n",
      "1 1 x sixty seven . y sixty\n",
      "1 2 x one hundred nineteen y .\n",
      "1 3 x hundred fifty four y .\n",
      "1 4 x one hundred eighty y nine\n",
      "2 0 x four . five y .\n",
      "2 1 x sixty eight . y sixty\n",
      "2 2 x . one hundred y twenty\n",
      "2 3 x . one hundred y fifty\n",
      "2 4 x nine . one y hundred\n"
     ]
    }
   ],
   "source": [
    "let _dataTemp = toData(nums,3);\n",
    "let _data=groupChunks(_dataTemp);\n",
    "\n",
    "console.log(shape(_data));\n",
    "let _batches=batches(_data,64,true,false);\n",
    "for (let b=0; b<3; b++) {\n",
    "    let _batch=_batches[b];\n",
    "    [...Array(5).keys()].forEach(i => {\n",
    "        console.log(b,i,'x',vocab[_batch[0][i]],vocab[_batch[1][i]],vocab[_batch[2][i]],'y',vocab[_batch[3][i]]);\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-cameroon",
   "metadata": {},
   "source": [
    "here's how we could move the initialization of `h` out of `forward` &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "turned-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3 extends LMModel2 {\n",
    "    constructor(vocab_sz, n_hidden, sequenceLength) {\n",
    "        super(vocab_sz, n_hidden, sequenceLength);\n",
    "        this.h=0;\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        for (let i=0; i<3; i++) {\n",
    "            this.h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), this.h);\n",
    "            this.h = this.non_linear.forward(this.h_h.forward(this.h));\n",
    "        }\n",
    "        return this.h_o.forward(this.h);\n",
    "    }\n",
    "        \n",
    "    reset() {\n",
    "        this.h=0;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-thing",
   "metadata": {},
   "source": [
    "without organising the data so the model sees contiguous text over subsequent batches, we loose a little accuracy (~2%) but i thought we'd loose more. Maybe if we were looking at validation accuracy we'd see more of a difference.\n",
    "\n",
    "organising the data with `groupChunks` improves ~2% over our fist model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concerned-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel3 extends LMModel2]\n",
      "epoch 0 2021-05-14T10:05:49.903Z train loss 2.786 accuracy 0.323\n",
      "epoch 1 2021-05-14T10:05:51.045Z train loss 2.328 accuracy 0.41\n",
      "epoch 2 2021-05-14T10:05:52.453Z train loss 2.104 accuracy 0.449\n",
      "epoch 3 2021-05-14T10:05:53.754Z train loss 1.967 accuracy 0.473\n",
      "epoch 4 2021-05-14T10:05:54.937Z train loss 1.871 accuracy 0.489\n",
      "epoch 5 2021-05-14T10:05:56.086Z train loss 1.799 accuracy 0.501\n",
      "epoch 6 2021-05-14T10:05:57.257Z train loss 1.742 accuracy 0.511\n",
      "epoch 7 2021-05-14T10:05:58.400Z train loss 1.695 accuracy 0.52\n",
      "epoch 8 2021-05-14T10:05:59.551Z train loss 1.657 accuracy 0.527\n",
      "epoch 9 2021-05-14T10:06:00.749Z train loss 1.626 accuracy 0.533\n",
      "epoch 10 2021-05-14T10:06:01.898Z train loss 1.599 accuracy 0.538\n",
      "epoch 11 2021-05-14T10:06:03.047Z train loss 1.576 accuracy 0.542\n",
      "epoch 12 2021-05-14T10:06:04.168Z train loss 1.556 accuracy 0.546\n",
      "epoch 13 2021-05-14T10:06:05.306Z train loss 1.54 accuracy 0.549\n",
      "epoch 14 2021-05-14T10:06:06.550Z train loss 1.526 accuracy 0.551\n"
     ]
    }
   ],
   "source": [
    "train(LMModel3,groupChunks(toData(nums,3)),true,false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-turkey",
   "metadata": {},
   "source": [
    "# Creating More Signal\n",
    "\n",
    "## This one is a work in progress\n",
    "\n",
    "I can't find a way to train this model to anything like the accuracy of the previous models - loss reduces for a few epochs, but then starts increasing and usually goes to infinity unless we stop training early.\n",
    "\n",
    "Here's some things I've tried;\n",
    "- normalizing gradients going in to `this.h_o.backward`\n",
    "    - seems strange that the model can train at all when we do this as it must be changing the sign of some gradients when we center to mean 0?\n",
    "- reset `h` at the start of forward\n",
    "    - with and without shuffling data in batches\n",
    "- different batch sizes\n",
    "    - up to putting all of the data into a single batch\n",
    "- different learning rates\n",
    "    - even `lr`s that look too slow to get anywhere near good accuracy can cause loss -> infinity\n",
    "- going backward over fewer timesteps that forward\n",
    "- swapping `i_h` `Embedding` for `Linear`\n",
    "\n",
    "There's probably a mistake in here somewhere but ... maybe this model is too deep to train with plain SDG? So we might need to use; Adam, one-cycle, weight decay ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "covered-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4 {\n",
    "    constructor(vocab_sz, n_hidden) {\n",
    "        this.useEmbedding=true;\n",
    "        if (this.useEmbedding) {\n",
    "            const embedding=new Embedding(vocab_sz, n_hidden);\n",
    "            embedding.weights=uniform(vocab_sz, n_hidden,-2,2);\n",
    "            this.i_h = new MultiCallLayer(embedding);\n",
    "        } else {\n",
    "            this.i_h = new MultiCallLayer(new Linear(vocab_sz, n_hidden));\n",
    "        }\n",
    "        this.h_h = new MultiCallLayer(new Linear(n_hidden, n_hidden));\n",
    "        this.h_o = new MultiCallLayer(new Linear(n_hidden, vocab_sz));\n",
    "        this.non_linear = new ReLU();\n",
    "        this.oneHotLookup = normalize(identity(vocab_sz));\n",
    "        this.h=0;\n",
    "    }\n",
    "    \n",
    "    toOneHot(x) {\n",
    "        return x.map(e=>this.oneHotLookup[e]);\n",
    "    }\n",
    "    \n",
    "    forward(x) {\n",
    "        const result = [];\n",
    "        for (let i=0; i<x.length-1; i++) {\n",
    "            if (this.useEmbedding) {\n",
    "                this.h = matrixSum2d(this.i_h.forward(x[i]), this.h);\n",
    "            } else {\n",
    "                this.h = matrixSum2d(this.i_h.forward(this.toOneHot(x[i])), this.h);\n",
    "            }\n",
    "            this.h = this.non_linear.forward(this.h_h.forward(this.h));\n",
    "            result.push(this.h_o.forward(this.h));\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "    \n",
    "    backward(gradients) {\n",
    "        for (let i=gradients.length-1; i>=0; i--) {\n",
    "            let g=this.h_o.backward(gradients[i]);\n",
    "            g=this.non_linear.backward(g);\n",
    "            g=this.h_h.backward(g);\n",
    "            // TODO: matrix sum\n",
    "            this.i_h.backward(g);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    update(lr) {\n",
    "        this.i_h.update(lr);\n",
    "        this.h_h.update(lr);\n",
    "        this.h_o.update(lr);\n",
    "    }\n",
    "    \n",
    "    reset() {\n",
    "        this.h=0;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "injured-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten {\n",
    "    forward(x) {\n",
    "        this.originalShape=shape(x);\n",
    "        return [].concat(...x);\n",
    "    }\n",
    "    backward(x) {\n",
    "        const result=[];\n",
    "        for (let i=0; i<this.originalShape[0]; i++) {\n",
    "            const startFrom=i*this.originalShape[1];\n",
    "            result.push(x.slice(startFrom,startFrom+this.originalShape[1]));\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-addiction",
   "metadata": {},
   "source": [
    "The following test shows that we can use the same flatten for multiple arrays, as long as the first 2 dimentions of the arrays are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optional-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "let a=[],b=[];\n",
    "for (let i=0; i<3; i++) {\n",
    "    a.push([]); b.push([]);\n",
    "    for (let j=0; j<2; j++) {\n",
    "        a[a.length-1].push(`${i}.${j}`);\n",
    "        let _b=[]; b[b.length-1].push(_b);\n",
    "        for (let k=0; k<4; k++) {\n",
    "            _b.push(`${i}.${j}.${k}`);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "let _a=JSON.stringify(a),_b=JSON.stringify(b); \n",
    "let flatten=new Flatten();\n",
    "let aFlat=flatten.forward(a);\n",
    "let bFlat=flatten.forward(b);\n",
    "testEq([6],shape(aFlat));\n",
    "testEq([6,4],shape(bFlat));\n",
    "let aUnflat=flatten.backward(aFlat);\n",
    "let bUnflat=flatten.backward(bFlat);\n",
    "testEq([3,2],shape(aUnflat));\n",
    "testEq([3,2,4],shape(bUnflat));\n",
    "testEq(_a,JSON.stringify(aUnflat));\n",
    "testEq(_b,JSON.stringify(bUnflat));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sticky-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train2(model_fn,data,dropLastBatch=false,shuffleBatch=true) {\n",
    "    console.log('Training model',model_fn);\n",
    "    let lossFn=new CrossEntropyLoss();\n",
    "    let model=new model_fn(vocab.length,48,data.length-1); // data.length-1 is sequence length\n",
    "    let flatten=new Flatten();\n",
    "    let yTrue=data[model.sequenceLength];\n",
    "    let lossValues=[];\n",
    "    let accuracyValues=[];\n",
    "    for (let epoch=0; epoch<20; epoch++) {\n",
    "        batches(data,64,dropLastBatch,shuffleBatch).forEach(batch => {\n",
    "            const xb=batch; // the model will look at only sequence length tokens\n",
    "            const ybFlat=flatten.forward(batch.filter((e,i)=>i!=0));\n",
    "            let predsFlat=flatten.forward(model.forward(xb));\n",
    "            let lossValue=lossFn.forward(predsFlat,ybFlat);\n",
    "            if (lossValue==Infinity) {\n",
    "                throw 'lossValue==Infinity';\n",
    "            }\n",
    "            lossValues.push(lossValue);\n",
    "            accuracyValues.push(accuracy(predsFlat,ybFlat));\n",
    "            model.backward(flatten.backward(lossFn.backward()));\n",
    "            model.update(3e-3);\n",
    "        });\n",
    "        console.log('epoch',epoch,new Date(),'train loss',round(mean(lossValues),3),\n",
    "                    'accuracy',round(mean(accuracyValues),3));\n",
    "        if (model.reset) {\n",
    "            model.reset();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "approximate-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [class LMModel4]\n",
      "epoch 0 2021-05-14T10:07:12.480Z train loss 4.831 accuracy 0.085\n",
      "epoch 1 2021-05-14T10:07:14.403Z train loss 4.346 accuracy 0.11\n",
      "epoch 2 2021-05-14T10:07:16.364Z train loss 4.078 accuracy 0.131\n",
      "epoch 3 2021-05-14T10:07:18.328Z train loss 3.897 accuracy 0.149\n",
      "epoch 4 2021-05-14T10:07:20.298Z train loss 3.764 accuracy 0.166\n",
      "epoch 5 2021-05-14T10:07:22.405Z train loss 3.66 accuracy 0.18\n",
      "epoch 6 2021-05-14T10:07:24.424Z train loss 3.576 accuracy 0.193\n",
      "epoch 7 2021-05-14T10:07:26.436Z train loss 3.508 accuracy 0.204\n",
      "epoch 8 2021-05-14T10:07:28.441Z train loss 3.453 accuracy 0.213\n",
      "epoch 9 2021-05-14T10:07:30.372Z train loss 3.407 accuracy 0.221\n",
      "epoch 10 2021-05-14T10:07:32.329Z train loss 3.369 accuracy 0.228\n",
      "epoch 11 2021-05-14T10:07:34.256Z train loss 3.339 accuracy 0.235\n",
      "epoch 12 2021-05-14T10:07:36.258Z train loss 3.316 accuracy 0.241\n",
      "epoch 13 2021-05-14T10:07:38.299Z train loss 3.298 accuracy 0.247\n",
      "epoch 14 2021-05-14T10:07:40.254Z train loss 3.287 accuracy 0.252\n",
      "epoch 15 2021-05-14T10:07:42.202Z train loss 3.281 accuracy 0.256\n",
      "epoch 16 2021-05-14T10:07:44.367Z train loss 3.282 accuracy 0.261\n",
      "epoch 17 2021-05-14T10:07:46.633Z train loss 3.289 accuracy 0.265\n",
      "epoch 18 2021-05-14T10:07:49.100Z train loss 3.303 accuracy 0.269\n",
      "epoch 19 2021-05-14T10:07:51.203Z train loss 3.323 accuracy 0.272\n"
     ]
    }
   ],
   "source": [
    "train2(LMModel4,groupChunks(toData(nums,8)),true,false);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
