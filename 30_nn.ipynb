{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "//default_exp nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-restaurant",
   "metadata": {},
   "source": [
    "# nn\n",
    "\n",
    "> A few modules that can be used to build neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marine-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Imports we need in nn.module.js\n",
    "*/\n",
    "import {exp,shape,transpose,dotProduct,randn,uniform,zeros,argmax,mean,round} from './src/util.module.js';\n",
    "import {matrixSum1d,matrixSum2d,matrixSubtract1d,matrixSubtract2d,matrixMultiply1d,matrixMultiply2d} from './src/util.module.js';\n",
    "import {head,tail,parseCsv,IRIS_CLASS_MAP,IrisRowHandler,shuffle,split,batches} from './src/data.module.js';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enormous-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Imports we need for testing\n",
    "import {testEq} from './src/testutil.module.js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compliant-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "yTrue can be either 2d (one-hot encoded targets) or 1d (array of class IDs).\n",
    "*/\n",
    "function accuracy(yPred2d,yTrue) {\n",
    "    const yPredShape=shape(yPred2d);\n",
    "    const yTrueShape=shape(yTrue);\n",
    "    if (yPredShape[0] != yTrueShape[0]) {\n",
    "        throw new Error(`Expected yPred2d.length ${yPredShape[0]} to equal yTrue.length ${yTrueShape[0]}`);\n",
    "    }\n",
    "    if (yTrueShape.length == 2 && yPredShape[1] != yTrueShape[1]) {\n",
    "        throw new Error(`Expected shape(yPred2d)[1] ${yPredShape[1]} to equal shape(yTrue)[1] ${yTrueShape[1]}`);\n",
    "    }\n",
    "    let correctCount=0;\n",
    "    for (let i=0; i<yPred2d.length; i++) {\n",
    "        let p = argmax(yPred2d[i]);\n",
    "        let t = (yTrueShape.length == 2) ? argmax(yTrue[i]) : yTrue[i];\n",
    "        if (p == t) {\n",
    "            correctCount++;\n",
    "        }\n",
    "    }\n",
    "    return correctCount/yPredShape[0];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recovered-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "let yPred=[[0,.1],[0,.9],[0,.33],[0,1],[0,1],[0,1],[0,1],[0,1],[1,0]];\n",
    "let yTrue=[[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[1,0]];\n",
    "testEq(1.0, accuracy(yPred,yTrue));\n",
    "yPred[0][0]=.8;\n",
    "testEq(8/9, accuracy(yPred,yTrue));\n",
    "yPred.map(a=>a[0]=1.1); // accuracy doesn't care if we're not within 0 and 1\n",
    "testEq(1/9, accuracy(yPred,yTrue));\n",
    "// test with class IDs\n",
    "testEq(1/9, accuracy(yPred,[1,1,1,1,1,1,1,1,0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daily-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "*/\n",
    "class MSE {\n",
    "    forward(yPred2d,yTrue2d) {\n",
    "        this.error=matrixSubtract2d(yPred2d,yTrue2d);\n",
    "        return mean(this.error.map(row=>row.map(elem=>elem**2)));\n",
    "    }\n",
    "    backward() {\n",
    "        this.grad=matrixMultiply2d(this.error, 2/this.error.length);\n",
    "        return this.grad;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intelligent-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "let mse=new MSE();\n",
    "let mseValue=mse.forward(\n",
    "    [[-0.1684, -1.0158, -1.3667,  1.4327],\n",
    "    [ 0.0245, -0.6284, -2.5182,  2.2007],\n",
    "    [-1.8774, -0.0352, -0.5946,  0.4272]],\n",
    "    [[-0.3516,  0.5787,  0.8858,  0.9198],\n",
    "    [ 0.1892, -0.6473,  2.1278,  0.1345],\n",
    "    [ 2.2919, -0.9939, -0.3137, -0.4314]]);\n",
    "testEq(4.409, Math.round(mseValue*1000)/1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-shakespeare",
   "metadata": {},
   "source": [
    "## cross entropy: negative log likelyhood of log softmax\n",
    "\n",
    "The following is taken from https://github.com/fastai/course-v3/blob/master/nbs/dl2/03_minibatch_training.ipynb\n",
    "```\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n",
    "\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twenty-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Takes a 2d array and returns a 1d array of the log of the sum of the exp for each row.\n",
    "*/\n",
    "function logsumexp(x) {\n",
    "    const m = x.map(a => Math.max(...a));\n",
    "    let temp = x.map((row,i) => row.map(e => e-m[i])); // x-m[:,None]\n",
    "    temp = temp.map(row => row.map(e => exp(e)));      // .exp()\n",
    "    temp = temp.map(row => row.reduce((a,b) => a+b))   // .sum(-1)\n",
    "    temp = temp.map(a => Math.log(a));                 // .log()\n",
    "    return matrixSum1d(m, temp);                       // return m + ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "let testData=[[1.6392130817141863, 0.12928212984246149],\n",
    "              [0.000843200027605633, -0.12680858189363003],\n",
    "              [-0.9898354893794594, -1.5028466126461082]]\n",
    "testEq([1.8388, 0.6322, -0.5207], round(logsumexp(testData),4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indie-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Takes a 2d array and returns a 2d array of log softmax for each element.\n",
    "*/\n",
    "function log_softmax(x) {\n",
    "    const _logsumexp = logsumexp(x);\n",
    "    return x.map((row,i) => row.map(e => e-_logsumexp[i]));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "raising-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEq([[-0.1996089581238054, -1.7095399099955302],\n",
    "        [-0.6313567803288485, -0.7590085622500842],\n",
    "        [-0.4691846265662769, -0.9821957498329257]], log_softmax(testData));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "angry-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Takes a 2d input (log softmax predictions) and a 1d array of target class IDs and returns the negative log likelihood.\n",
    "*/\n",
    "function nll(input, target) {\n",
    "    return -mean(input.map((row,i) => row[target[i]]));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "residential-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEq(-0.2167, round(nll(testData,[0,0,0]),4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-rugby",
   "metadata": {},
   "source": [
    "`CrossEntropyLoss` uses an approach borrowed from https://beckernick.github.io/logistic-regression-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "simple-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Cross entropy with softmax.\n",
    "yTrue1d is an array of target class IDs - not a 2d array of 1 hot encoded targets.\n",
    "*/\n",
    "class CrossEntropyLoss {\n",
    "    softmax1d(a) {\n",
    "        const maxValue=Math.max(...a); // normalize values for numerical stability (log sum exp)\n",
    "        const temp=a.map(e => exp(e-maxValue));\n",
    "        const sum=temp.reduce((a,b)=>a+b);\n",
    "        return temp.map(e=>e/sum);\n",
    "    }\n",
    "        \n",
    "    forward(yPred2d,yTrue1d) {\n",
    "        this.yPred2d=yPred2d.map(yPred1d => this.softmax1d(yPred1d));\n",
    "        this.yTrue1d=yTrue1d;\n",
    "        const temp=this.yPred2d.map((yPred1d,i) => Math.log(yPred1d[yTrue1d[i]])); // TODO: add tiny value to avoid log(0)\n",
    "        return -temp.reduce((a,b) => a+b) / temp.length;\n",
    "    }\n",
    "    \n",
    "    backward() {\n",
    "        const yTrue1d=this.yTrue1d;\n",
    "        this.grad=this.yPred2d.map(yPred1d => [...yPred1d]); // copy preds\n",
    "        this.grad.forEach((yPred1d,i)=>yPred1d[yTrue1d[i]]-=1);\n",
    "        return this.grad;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coated-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "// show that both ways of calculating cross entropy loss give the same values\n",
    "let lossFn = new CrossEntropyLoss()\n",
    "testEq(round(nll(log_softmax(testData),[0,0,0]),4), round(lossFn.forward(testData,[0,0,0]),4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suitable-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "*/\n",
    "class BinaryCrossEntropyLoss {\n",
    "    _forward1d(yPred1d,yTrue1d) {\n",
    "        const temp=yPred1d.map((yPred,i) => Math.log((yTrue1d[i]==1.) ? yPred : 1-yPred));\n",
    "        return -temp.reduce((a,b) => a+b) / temp.length;\n",
    "    }\n",
    "    forward(yPred2d,yTrue2d) {\n",
    "        this.yPred2d=yPred2d;\n",
    "        this.yTrue2d=yTrue2d;\n",
    "        const lossValue1d=yPred2d.map((yPred1d,i) => this._forward1d(yPred1d,yTrue2d[i]));\n",
    "        return lossValue1d.reduce((a,b) => a+b) / lossValue1d.length;\n",
    "    }\n",
    "    _backward1d(yPred1d,yTrue1d) {\n",
    "        return yPred1d.map((yPred,i) => (yTrue1d[i]==1.) ? -1/yPred : 1/(1-yPred));\n",
    "    }\n",
    "    backward() {\n",
    "        const yTrue2d=this.yTrue2d;\n",
    "        this.grad=this.yPred2d.map((yPred1d,i) => this._backward1d(yPred1d,yTrue2d[i]));\n",
    "        return this.grad;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "covered-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "*/\n",
    "class Sigmoid {\n",
    "    forward(x2d) {\n",
    "        this.results=x2d.map(x1d => x1d.map(x => 1./(1.+exp(-x))));\n",
    "        return this.results;\n",
    "    }\n",
    "    backward(gradients) {\n",
    "        // `s * (1.-s)` calculates sigmoid grad, then we chain gradients passed in\n",
    "        this.grad=this.results.map((result,i) => result.map((s,j) => s * (1.-s) * gradients[i][j]));\n",
    "        return this.grad;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "passing-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "*/\n",
    "class ReLU {\n",
    "    forward(x2d) {\n",
    "        this.gradMask=zeros(...shape(x2d));\n",
    "        return x2d.map((x1d,rowIndex) => x1d.map((x,colIndex) => {\n",
    "            if (x>0) {\n",
    "                this.gradMask[rowIndex][colIndex]=1;\n",
    "            }\n",
    "            return Math.max(0,x);\n",
    "        }));\n",
    "    }\n",
    "    backward(gradient) {\n",
    "        return matrixMultiply2d(this.gradMask,gradient);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "attempted-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "let relu = new ReLU();\n",
    "let data = [\n",
    "  [ -0.3132450550822199, 0.06746248970796562, 0.7502210053477679 ],\n",
    "  [ 0.32586239499711434, 0.276573231917191, 0.4718188033994297 ],\n",
    "  [ 0.3375259522729109, -1.4738907605515226, -0.11109898767917284 ],\n",
    "  [ -0.6095143988686595, 1.094470501593892, -0.4982351760328258 ],\n",
    "  [ 0.28664244098736347, -0.35879217465991975, -0.754257906608068 ]\n",
    "];\n",
    "testEq(relu.forward(data),relu.backward(data));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "imported-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Applies a linear transformation to `x`.\n",
    "*/\n",
    "class Linear {\n",
    "    constructor(inputDim,numHidden=1,bias=true) {\n",
    "        this.inputDim=inputDim;\n",
    "        this.numHidden=numHidden;\n",
    "        // Kaiming Init\n",
    "        this.weights=matrixMultiply2d(randn(inputDim,numHidden), Math.sqrt(2.0/inputDim));\n",
    "        this.bias=zeros(numHidden)\n",
    "        this.updateBias=bias;\n",
    "    }\n",
    "    forward(x) {\n",
    "        this.x=x; // shape(bs,inputDim)\n",
    "        return matrixSum2d(dotProduct(x,this.weights), this.bias);\n",
    "    }\n",
    "    backward(gradient) { // gradient shape(bs,numHidden)\n",
    "        // weightsGradient/biasGradient need to be the same shape as weights/bias\n",
    "        this.weightsGradient=dotProduct(transpose(this.x), gradient);\n",
    "        // this.biasGradient=gradient.sum(axis=0)\n",
    "        this.biasGradient=transpose(gradient).map(col => col.reduce((a,b) => a+b));\n",
    "        this.xGradient=dotProduct(gradient,transpose(this.weights));\n",
    "        return this.xGradient;\n",
    "    }\n",
    "    update(lr) {\n",
    "        // gradient calculations in backward don't account for batch size, so we do it here\n",
    "        lr=lr/this.x.length; // TODO: change gradient calc to account for batch size - all XxxLoss classes\n",
    "        this.weights=matrixSubtract2d(this.weights,matrixMultiply2d(this.weightsGradient,lr));\n",
    "        if (this.updateBias) {\n",
    "            this.bias=matrixSubtract1d(this.bias,matrixMultiply1d(this.biasGradient,lr));\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-green",
   "metadata": {},
   "source": [
    "TODO: There might be some mistakes in `Embedding` - swapping \"input to hidden\" layer from `Linear` to `Embedding` in an RNN might hurt accuracy (see 35_rnn.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "possible-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Using\n",
    "- `Embedding` when `x` is an array of IDs or\n",
    "- `Linear` when `x` is a one-hot encoded matrix\n",
    "should give the same results - but `Embedding` should be faster.\n",
    "*/\n",
    "class Embedding extends Linear {\n",
    "    constructor(inputDim,numHidden=1,bias=true) {\n",
    "        super(inputDim,numHidden,bias);\n",
    "        this.weights=uniform(inputDim,numHidden,-1,1);\n",
    "    }\n",
    "    forward(x) {\n",
    "        this.x=x;\n",
    "        return matrixSum2d(x.map(i=>this.weights[i]), this.bias);\n",
    "    }\n",
    "    backward(gradient) { // gradient shape(bs,numHidden)\n",
    "        this.weightsGradient=zeros(this.inputDim,this.numHidden);\n",
    "        for (let i=0; i<this.inputDim; i++) {\n",
    "            this.x.map((row, rowIndex)=>{\n",
    "                if (row == i) {\n",
    "                    this.weightsGradient[i]=matrixSum1d(this.weightsGradient[i],gradient[rowIndex]);\n",
    "                }\n",
    "            })\n",
    "        }\n",
    "        this.biasGradient=transpose(gradient).map(col => col.reduce((a,b) => a+b));\n",
    "        this.xGradient=dotProduct(gradient,transpose(this.weights));\n",
    "        return this.xGradient;\n",
    "    }\n",
    "    update(lr) {\n",
    "        // gradient calculations in backward don't account for batch size, so we do it here\n",
    "        lr=lr/this.x.length; // TODO: change gradient calc to account for batch size - all XxxLoss classes\n",
    "        this.weights=matrixSubtract2d(this.weights,matrixMultiply2d(this.weightsGradient,lr));\n",
    "        if (this.updateBias) {\n",
    "            this.bias=matrixSubtract1d(this.bias,matrixMultiply1d(this.biasGradient,lr));\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-argentina",
   "metadata": {},
   "source": [
    "The following shows that `Linear` and `Embedding` apply the same transformation and calculate the same gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hearing-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "var inputDim=3;\n",
    "var numHidden=5\n",
    "let linear=new Linear(inputDim, numHidden);\n",
    "let embedding=new Embedding(inputDim, numHidden);\n",
    "linear.weights=embedding.weights;\n",
    "\n",
    "let embedding_in=[0,1,2,1];\n",
    "let bs=embedding_in.length;\n",
    "let linear_in=zeros(bs, inputDim);\n",
    "embedding_in.forEach((e,i)=>linear_in[i][e]=1);\n",
    "testEq(linear.forward(linear_in),embedding.forward(embedding_in));\n",
    "\n",
    "let gradient=randn(bs,numHidden);\n",
    "linear.backward(gradient);\n",
    "embedding.backward(gradient);\n",
    "testEq(linear.weightsGradient, embedding.weightsGradient);\n",
    "testEq(linear.biasGradient, embedding.biasGradient);\n",
    "testEq(linear.xGradient, embedding.xGradient);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accomplished-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "*/\n",
    "class Learner {\n",
    "    constructor(model, lossFn, data, metrics=[accuracy]) {\n",
    "        this.model=model;\n",
    "        this.lossFn=lossFn;\n",
    "        this.metrics=metrics;\n",
    "        const splitData=split(shuffle(data));\n",
    "        this.xTrain=splitData[0][0];\n",
    "        this.xValid=splitData[0][1];\n",
    "        this.yTrain=splitData[1][0];\n",
    "        this.yValid=splitData[1][1];\n",
    "        // shame that we can destructure into this. )o:\n",
    "//         [[this.xTrain,this.xValid],[this.yTrain,this.yValid]]=split(data);\n",
    "    }\n",
    "    forward(x) {\n",
    "        for (let i=0; i<this.model.length; i++) {\n",
    "            x=this.model[i].forward(x);\n",
    "        }\n",
    "        return x;\n",
    "    }\n",
    "    backward(gradients) {\n",
    "        for (let i=this.model.length-1; i>=0; i--) {\n",
    "            gradients=this.model[i].backward(gradients);\n",
    "        }\n",
    "        return gradients;\n",
    "    }\n",
    "    step(lr) {\n",
    "        this.model.forEach(m => {\n",
    "            if (typeof m.update=='function') {\n",
    "                m.update(lr);\n",
    "            }\n",
    "        });\n",
    "    }\n",
    "    validate(epoch) {\n",
    "        const preds=this.forward(this.xValid);\n",
    "        const lossValue=this.lossFn.forward(preds,this.yValid);\n",
    "        const metricValues=this.metrics.map(metric=>metric(preds,this.yValid));\n",
    "        console.log('epoch',epoch,'valid loss',lossValue,'metrics',metricValues);\n",
    "    }\n",
    "    fit(epochs, lr=0.1, bs=64) {\n",
    "        this.validate(-1); // Note: we use epoch -1 to indicate before training\n",
    "        for (let epoch=0; epoch<epochs; epoch++) {\n",
    "            batches([this.xTrain,this.yTrain]).forEach(batch => {\n",
    "                const [xb,yb]=batch;\n",
    "                const preds=this.forward(xb);\n",
    "                const lossValue=this.lossFn.forward(preds,yb);\n",
    "                this.lossFn.backward();\n",
    "                this.backward(this.lossFn.grad);\n",
    "                this.step(lr);\n",
    "            });\n",
    "            this.validate(epoch);\n",
    "        }\n",
    "    }\n",
    "    predict(x,y,yToLabelFn=(a=>a)) {\n",
    "        const preds=this.forward(x);\n",
    "        return preds.map((pred,rowIndex) => {\n",
    "            const row=[pred,yToLabelFn(pred)];\n",
    "            if (y!=null) {\n",
    "                row.push(yToLabelFn(y[rowIndex]));\n",
    "            }\n",
    "            return row;\n",
    "        });\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-compilation",
   "metadata": {},
   "source": [
    "## Train a linear model to classify iris flowers\n",
    "\n",
    "Note: we use `BinaryCrossEntropyLoss` here just as an example. README.md and index.ipynb shows how to train with `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "agreed-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch -1 valid loss 0.8946537959087612 metrics [ 0.36666666666666664 ]\n",
      "epoch 0 valid loss 0.8284019027485837 metrics [ 0.4 ]\n",
      "epoch 1 valid loss 0.7744554242469264 metrics [ 0.4666666666666667 ]\n",
      "epoch 2 valid loss 0.7292967866647169 metrics [ 0.4 ]\n",
      "epoch 3 valid loss 0.6914984356688229 metrics [ 0.3333333333333333 ]\n",
      "epoch 4 valid loss 0.6595268742408502 metrics [ 0.36666666666666664 ]\n",
      "epoch 5 valid loss 0.631654681021758 metrics [ 0.4666666666666667 ]\n",
      "epoch 6 valid loss 0.6073176486056152 metrics [ 0.43333333333333335 ]\n",
      "epoch 7 valid loss 0.585114575972231 metrics [ 0.4 ]\n",
      "epoch 8 valid loss 0.5652309678962602 metrics [ 0.4666666666666667 ]\n",
      "epoch 9 valid loss 0.5475742644725528 metrics [ 0.5 ]\n",
      "epoch 10 valid loss 0.5311732787120413 metrics [ 0.5666666666666667 ]\n",
      "epoch 11 valid loss 0.516261371104263 metrics [ 0.6333333333333333 ]\n",
      "epoch 12 valid loss 0.5026456052979619 metrics [ 0.6333333333333333 ]\n",
      "epoch 13 valid loss 0.49017694042313603 metrics [ 0.6333333333333333 ]\n",
      "epoch 14 valid loss 0.47851024264409153 metrics [ 0.6333333333333333 ]\n",
      "epoch 15 valid loss 0.46777356064297454 metrics [ 0.6333333333333333 ]\n",
      "epoch 16 valid loss 0.4579288131113597 metrics [ 0.6333333333333333 ]\n",
      "epoch 17 valid loss 0.448517800505358 metrics [ 0.6666666666666666 ]\n",
      "epoch 18 valid loss 0.4398723725652548 metrics [ 0.7 ]\n",
      "epoch 19 valid loss 0.4317438300976928 metrics [ 0.7333333333333333 ]\n",
      "epoch 20 valid loss 0.42444019375483955 metrics [ 0.7333333333333333 ]\n",
      "epoch 21 valid loss 0.41751480623546267 metrics [ 0.7333333333333333 ]\n",
      "epoch 22 valid loss 0.4109758819935911 metrics [ 0.7333333333333333 ]\n",
      "epoch 23 valid loss 0.40485031223269724 metrics [ 0.7333333333333333 ]\n",
      "epoch 24 valid loss 0.3990830588540204 metrics [ 0.7333333333333333 ]\n"
     ]
    }
   ],
   "source": [
    "let stringData=require('fs').readFileSync('data/iris.data').toString();\n",
    "let data=parseCsv(stringData, new IrisRowHandler()).result;\n",
    "let lossFn=new BinaryCrossEntropyLoss();\n",
    "let model=[new Linear(4,3), new Sigmoid()];\n",
    "let learn=new Learner(model, lossFn, data);\n",
    "learn.fit(25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-cleaner",
   "metadata": {},
   "source": [
    "## Train a neural net to classify iris flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "disabled-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch -1 valid loss 0.8568930157856347 metrics [ 0.26666666666666666 ]\n",
      "epoch 0 valid loss 0.437616882101267 metrics [ 0.6333333333333333 ]\n",
      "epoch 1 valid loss 0.3804940434785776 metrics [ 0.6333333333333333 ]\n",
      "epoch 2 valid loss 0.35520078060417337 metrics [ 0.6666666666666666 ]\n",
      "epoch 3 valid loss 0.33558010528363347 metrics [ 0.6666666666666666 ]\n",
      "epoch 4 valid loss 0.3230524279055066 metrics [ 0.7333333333333333 ]\n",
      "epoch 5 valid loss 0.31307829001348497 metrics [ 0.7333333333333333 ]\n",
      "epoch 6 valid loss 0.3052917886013161 metrics [ 0.7666666666666667 ]\n",
      "epoch 7 valid loss 0.2965778376726197 metrics [ 0.7666666666666667 ]\n",
      "epoch 8 valid loss 0.29116521706750237 metrics [ 0.7666666666666667 ]\n",
      "epoch 9 valid loss 0.28662934572742615 metrics [ 0.7666666666666667 ]\n",
      "epoch 10 valid loss 0.2816463763012414 metrics [ 0.7666666666666667 ]\n",
      "epoch 11 valid loss 0.2813482431523697 metrics [ 0.7666666666666667 ]\n",
      "epoch 12 valid loss 0.2775612231446735 metrics [ 0.7666666666666667 ]\n",
      "epoch 13 valid loss 0.27369192009599813 metrics [ 0.7666666666666667 ]\n",
      "epoch 14 valid loss 0.2721690952531981 metrics [ 0.7666666666666667 ]\n",
      "epoch 15 valid loss 0.27024542168648974 metrics [ 0.7666666666666667 ]\n",
      "epoch 16 valid loss 0.26489878339068895 metrics [ 0.7666666666666667 ]\n",
      "epoch 17 valid loss 0.26394159220705593 metrics [ 0.7666666666666667 ]\n",
      "epoch 18 valid loss 0.2602919833120765 metrics [ 0.7666666666666667 ]\n",
      "epoch 19 valid loss 0.25714866215312654 metrics [ 0.7666666666666667 ]\n",
      "epoch 20 valid loss 0.25414133465324806 metrics [ 0.7666666666666667 ]\n",
      "epoch 21 valid loss 0.2534978081055265 metrics [ 0.7666666666666667 ]\n",
      "epoch 22 valid loss 0.25128771044221265 metrics [ 0.7666666666666667 ]\n",
      "epoch 23 valid loss 0.2508401621500872 metrics [ 0.7666666666666667 ]\n",
      "epoch 24 valid loss 0.24908814830463352 metrics [ 0.7666666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "let model=[new Linear(4,50), new ReLU(), new Linear(50,3), new Sigmoid()];\n",
    "let learn=new Learner(model, lossFn, data);\n",
    "learn.fit(25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-coordination",
   "metadata": {},
   "source": [
    "### Look at some predictions \n",
    "\n",
    "We use the lambda ```(y=>`${argmax(y)}: ${IRIS_CLASS_MAP[argmax(y)]}`)``` to convert predictions like `[0.000, 0.183, 0.843]` to readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sensitive-checkout",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [\n",
      "  [ 0.002608827806792518, 0.010605660124589387, 0.9689722709781758 ],\n",
      "  '2: Iris-virginica',\n",
      "  '2: Iris-virginica'\n",
      "]\n",
      "1 [\n",
      "  [ 0.06538953197332577, 0.29118518818391553, 0.623852628082158 ],\n",
      "  '2: Iris-virginica',\n",
      "  '1: Iris-versicolor'\n",
      "]\n",
      "2 [\n",
      "  [ 0.9906443739497407, 0.012267864977518094, 0.003568835630294565 ],\n",
      "  '0: Iris-setosa',\n",
      "  '0: Iris-setosa'\n",
      "]\n",
      "3 [\n",
      "  [ 0.0006604101502350078, 0.02120899342452389, 0.9623130266561337 ],\n",
      "  '2: Iris-virginica',\n",
      "  '2: Iris-virginica'\n",
      "]\n",
      "4 [\n",
      "  [ 0.007161605576522388, 0.06898030177447727, 0.8866195218293936 ],\n",
      "  '2: Iris-virginica',\n",
      "  '2: Iris-virginica'\n",
      "]\n",
      "5 [\n",
      "  [ 0.09121717033286535, 0.4173399038250363, 0.5666338115920981 ],\n",
      "  '2: Iris-virginica',\n",
      "  '1: Iris-versicolor'\n",
      "]\n",
      "6 [\n",
      "  [ 0.9944036601487822, 0.003589683057275509, 0.004431618315949805 ],\n",
      "  '0: Iris-setosa',\n",
      "  '0: Iris-setosa'\n",
      "]\n",
      "7 [\n",
      "  [ 0.9892950303043117, 0.005505921099264821, 0.0019908417710695746 ],\n",
      "  '0: Iris-setosa',\n",
      "  '0: Iris-setosa'\n",
      "]\n",
      "8 [\n",
      "  [ 0.04262464369829163, 0.45920083581491045, 0.6078156210057865 ],\n",
      "  '2: Iris-virginica',\n",
      "  '1: Iris-versicolor'\n",
      "]\n",
      "9 [\n",
      "  [ 0.9867212747948948, 0.025475631646655728, 0.001860273898414739 ],\n",
      "  '0: Iris-setosa',\n",
      "  '0: Iris-setosa'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "// head(learn.predict(learn.xValid, learn.yValid)); run this to see \"raw\" targets\n",
    "head(learn.predict(learn.xValid, learn.yValid, (y=>`${argmax(y)}: ${IRIS_CLASS_MAP[argmax(y)]}`)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-liabilities",
   "metadata": {},
   "source": [
    "Show how we could train a linear layer without `Learner` - this is not a proper training loop, we just;\n",
    "- forward pass\n",
    "- print training loss\n",
    "- backward pass\n",
    "- update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "western-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) [ 150, 4 ] shape(y) [ 150, 3 ]\n",
      "epoch 0 loss_value 0.6583120472601168\n",
      "epoch 1 loss_value 0.6309250677200363\n",
      "epoch 2 loss_value 0.6060086737213931\n",
      "epoch 3 loss_value 0.58335342142375\n",
      "epoch 4 loss_value 0.5627671538988499\n",
      "epoch 5 loss_value 0.5440712925991019\n",
      "epoch 6 loss_value 0.5270984022695612\n",
      "epoch 7 loss_value 0.5116908605387391\n",
      "epoch 8 loss_value 0.4977003588368037\n",
      "epoch 9 loss_value 0.48498792886754294\n"
     ]
    }
   ],
   "source": [
    "let data=parseCsv(stringData, new IrisRowHandler()).result;\n",
    "let x=data[0],y=data[1];\n",
    "console.log('shape(x)',shape(x), 'shape(y)',shape(y));\n",
    "let loss_fn=new BinaryCrossEntropyLoss()\n",
    "let sig=new Sigmoid()\n",
    "let lin=new Linear(4,3);\n",
    "for (let epoch = 0; epoch < 10; epoch++) {\n",
    "    let y_pred=sig.forward(lin.forward(x));\n",
    "    let loss_value=loss_fn.forward(y_pred,y);\n",
    "    console.log('epoch',epoch,'loss_value',loss_value);\n",
    "    loss_fn.backward();\n",
    "    sig.backward(loss_fn.grad);\n",
    "    lin.backward(sig.grad);\n",
    "    lin.update(.1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-november",
   "metadata": {},
   "source": [
    "## Can we teach a linear layer to convert one hot encoded integers to their bitwise representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regulated-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "let x=[\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "];\n",
    "let y=[\n",
    "    [0,0,0,0],\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [1,1,0,0],\n",
    "    [0,0,1,0],\n",
    "    [1,0,1,0],\n",
    "    [0,1,1,0],\n",
    "    [1,1,1,0],\n",
    "    [0,0,0,1],\n",
    "    [1,0,0,1]\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-fight",
   "metadata": {},
   "source": [
    "`x` is an identity matrix, so ... `x.y` is `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sixth-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEq(y,dotProduct(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-count",
   "metadata": {},
   "source": [
    "so ... will `y` make the perfect weights (if bias is zero)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adult-beast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 loss_value 0.018148359589451796\n",
      "[\n",
      "  [\n",
      "    0.018479100169016396,\n",
      "    0.018081717857339245,\n",
      "    0.018075620023903786,\n",
      "    0.01774345249421659\n",
      "  ],\n",
      "  [\n",
      "    0.9819944022099567,\n",
      "    0.017751200326572868,\n",
      "    0.017550530926424546,\n",
      "    0.01839645897051965\n",
      "  ],\n",
      "  [\n",
      "    0.017776293901609837,\n",
      "    0.9816586779703848,\n",
      "    0.018510069720681404,\n",
      "    0.018227278750270474\n",
      "  ],\n",
      "  [\n",
      "    0.9821907768445811,\n",
      "    0.98176283372068,\n",
      "    0.018176082406075118,\n",
      "    0.01756385687442749\n",
      "  ],\n",
      "  [\n",
      "    0.017855907554515296,\n",
      "    0.018003349167369263,\n",
      "    0.982082493824773,\n",
      "    0.018115781969561887\n",
      "  ],\n",
      "  [\n",
      "    0.9815131486540767,\n",
      "    0.017492324342315617,\n",
      "    0.9817649375942173,\n",
      "    0.01796508139576266\n",
      "  ],\n",
      "  [\n",
      "    0.01786110353230478,\n",
      "    0.9821622448620041,\n",
      "    0.981732650775912,\n",
      "    0.017537121776692692\n",
      "  ],\n",
      "  [\n",
      "    0.981831120930172,\n",
      "    0.9825676292914914,\n",
      "    0.9825431420820094,\n",
      "    0.017755604589588043\n",
      "  ],\n",
      "  [\n",
      "    0.017788212427637236,\n",
      "    0.018469150357547346,\n",
      "    0.017617809653239242,\n",
      "    0.9822080680336192\n",
      "  ],\n",
      "  [\n",
      "    0.9815741081301447,\n",
      "    0.018132441612857025,\n",
      "    0.017810931792030138,\n",
      "    0.9817654319570406\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "let loss_fn=new BinaryCrossEntropyLoss()\n",
    "let sig=new Sigmoid()\n",
    "let linearNoBias=new Linear(10,4,false);\n",
    "let y_pred=null;\n",
    "for (let epoch = 0; epoch<10; epoch++) {\n",
    "    y_pred=sig.forward(linearNoBias.forward(x));\n",
    "    const loss_value=loss_fn.forward(y_pred,y);\n",
    "    if (epoch%10==9) {\n",
    "        console.log('epoch',epoch,'loss_value',loss_value);\n",
    "    }\n",
    "    loss_fn.backward();\n",
    "    sig.backward(loss_fn.grad);\n",
    "    linearNoBias.backward(sig.grad);\n",
    "    linearNoBias.update(50);\n",
    "}\n",
    "console.log(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-skirt",
   "metadata": {},
   "source": [
    "dump our linear layer to output - so we can look at the learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "statewide-cholesterol",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear {\n",
      "  inputDim: 10,\n",
      "  numHidden: 4,\n",
      "  weights: [\n",
      "    [\n",
      "      -4.064858435225292,\n",
      "      -4.08501531375404,\n",
      "      -4.085328329074123,\n",
      "      -4.102553212406025\n",
      "    ],\n",
      "    [\n",
      "      4.088930899006188,\n",
      "      -4.102147500386009,\n",
      "      -4.112717364412342,\n",
      "      -4.069011596808315\n",
      "    ],\n",
      "    [\n",
      "      -4.1008347921565225,\n",
      "      4.071793730917484,\n",
      "      -4.063307209517691,\n",
      "      -4.077576926175493\n",
      "    ],\n",
      "    [\n",
      "      4.099115201134334,\n",
      "      4.077073981975715,\n",
      "      -4.080185818007658,\n",
      "      -4.112011427865646\n",
      "    ],\n",
      "    [\n",
      "      -4.0966831601069105,\n",
      "      -4.0890468381859995,\n",
      "      4.0934846081001615,\n",
      "      -4.083268816625162\n",
      "    ],\n",
      "    [\n",
      "      4.064469925513472,\n",
      "      -4.115807602096847,\n",
      "      4.077180974081641,\n",
      "      -4.09102232200929\n",
      "    ],\n",
      "    [\n",
      "      -4.096412896982856,\n",
      "      4.0976280030354975,\n",
      "      4.075540496819237,\n",
      "      -4.113428290284634\n",
      "    ],\n",
      "    [\n",
      "      4.080553525055102,\n",
      "      4.119002165408427,\n",
      "      4.1176959675464335,\n",
      "      -4.101916957892983\n",
      "    ],\n",
      "    [\n",
      "      -4.100212002099947,\n",
      "      -4.065357404191012,\n",
      "      -4.109159174254026,\n",
      "      4.100017733421677\n",
      "    ],\n",
      "    [\n",
      "      4.067530132864117,\n",
      "      -4.082415949840487,\n",
      "      -4.099026068191563,\n",
      "      4.077206116743727\n",
      "    ]\n",
      "  ],\n",
      "  bias: [ 0, 0, 0, 0 ],\n",
      "  updateBias: false,\n",
      "  x: [\n",
      "    [\n",
      "      1, 0, 0, 0, 0,\n",
      "      0, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 1, 0, 0, 0,\n",
      "      0, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 1, 0, 0,\n",
      "      0, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 1, 0,\n",
      "      0, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 1,\n",
      "      0, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 0,\n",
      "      1, 0, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 0,\n",
      "      0, 1, 0, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 0,\n",
      "      0, 0, 1, 0, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 0,\n",
      "      0, 0, 0, 1, 0\n",
      "    ],\n",
      "    [\n",
      "      0, 0, 0, 0, 0,\n",
      "      0, 0, 0, 0, 1\n",
      "    ]\n",
      "  ],\n",
      "  weightsGradient: [\n",
      "    [\n",
      "      0.018479100169016396,\n",
      "      0.018081717857339242,\n",
      "      0.018075620023903782,\n",
      "      0.017743452494216588\n",
      "    ],\n",
      "    [\n",
      "      -0.018005597790043293,\n",
      "      0.017751200326572868,\n",
      "      0.017550530926424546,\n",
      "      0.01839645897051965\n",
      "    ],\n",
      "    [\n",
      "      0.01777629390160984,\n",
      "      -0.018341322029615248,\n",
      "      0.0185100697206814,\n",
      "      0.018227278750270474\n",
      "    ],\n",
      "    [\n",
      "      -0.017809223155418885,\n",
      "      -0.01823716627932004,\n",
      "      0.018176082406075118,\n",
      "      0.01756385687442749\n",
      "    ],\n",
      "    [\n",
      "      0.017855907554515296,\n",
      "      0.018003349167369263,\n",
      "      -0.017917506175227027,\n",
      "      0.018115781969561883\n",
      "    ],\n",
      "    [\n",
      "      -0.018486851345923316,\n",
      "      0.017492324342315617,\n",
      "      -0.018235062405782654,\n",
      "      0.01796508139576266\n",
      "    ],\n",
      "    [\n",
      "      0.01786110353230478,\n",
      "      -0.017837755137995903,\n",
      "      -0.018267349224088014,\n",
      "      0.017537121776692692\n",
      "    ],\n",
      "    [\n",
      "      -0.018168879069828003,\n",
      "      -0.01743237070850856,\n",
      "      -0.017456857917990565,\n",
      "      0.017755604589588043\n",
      "    ],\n",
      "    [\n",
      "      0.017788212427637236,\n",
      "      0.018469150357547346,\n",
      "      0.017617809653239242,\n",
      "      -0.01779193196638085\n",
      "    ],\n",
      "    [\n",
      "      -0.018425891869855326,\n",
      "      0.018132441612857025,\n",
      "      0.017810931792030135,\n",
      "      -0.01823456804295942\n",
      "    ]\n",
      "  ],\n",
      "  biasGradient: [\n",
      "    -0.0011358256459852757,\n",
      "    0.03608156950856162,\n",
      "    0.035864268799265964,\n",
      "    0.10727813681169923\n",
      "  ],\n",
      "  xGradient: [\n",
      "    [\n",
      "      -0.2890674020752055,\n",
      "      -0.14199284848137078,\n",
      "      -0.14467758539053996,\n",
      "      0.0026619424592484037,\n",
      "      -0.14483372828998356,\n",
      "      0.0016147059278347337,\n",
      "      -0.000981892419269173,\n",
      "      0.14827408360383876,\n",
      "      -0.14747687055383799,\n",
      "      -0.0004726650744469457\n",
      "    ],\n",
      "    [\n",
      "      -0.1433363602697592,\n",
      "      -0.287048677877981,\n",
      "      -0.0001363391039192613,\n",
      "      -0.14549470558080912,\n",
      "      -0.0020127230471648078,\n",
      "      -0.1466781923779594,\n",
      "      0.13917045257462707,\n",
      "      -0.003358773787579414,\n",
      "      0.004916955105916124,\n",
      "      -0.13948611551119527\n",
      "    ],\n",
      "    [\n",
      "      -0.1444574143055947,\n",
      "      -0.0022962197633450837,\n",
      "      -0.29047867011616413,\n",
      "      -0.14901468466256437,\n",
      "      0.003447256316533795,\n",
      "      0.1453435755155756,\n",
      "      -0.14438203799893723,\n",
      "      -0.0015724403291112604,\n",
      "      0.0002451402034550493,\n",
      "      0.14231162086623073\n",
      "    ],\n",
      "    [\n",
      "      0.0004858012178716453,\n",
      "      -0.1410345298867342,\n",
      "      -0.14332581348386123,\n",
      "      -0.28729779882821593,\n",
      "      0.14694785128862076,\n",
      "      0.0049002633108177485,\n",
      "      -0.000029455757699220486,\n",
      "      -0.14181206547693187,\n",
      "      0.1412564413901664,\n",
      "      -0.0008759738064937073\n",
      "    ],\n",
      "    [\n",
      "      -0.14398246210053037,\n",
      "      -0.0007801134941933935,\n",
      "      -0.001053596434743731,\n",
      "      0.14193977576362632,\n",
      "      -0.2876223141267942,\n",
      "      -0.14550350954109798,\n",
      "      -0.14370167988842797,\n",
      "      -0.0010895697764847223,\n",
      "      0.001558603237497319,\n",
      "      0.1431782477634932\n",
      "    ],\n",
      "    [\n",
      "      0.0043026524838628705,\n",
      "      -0.14218284191291952,\n",
      "      0.14457963002235152,\n",
      "      -0.003960809122087183,\n",
      "      -0.14060836861364834,\n",
      "      -0.2884624576578062,\n",
      "      -0.0007792814612590931,\n",
      "      -0.14882207725008978,\n",
      "      0.1500420105962454,\n",
      "      0.00141379871424617\n",
      "    ],\n",
      "    [\n",
      "      0.002888265993920225,\n",
      "      0.14679450251013035,\n",
      "      -0.14002922396117545,\n",
      "      0.002826249607064374,\n",
      "      -0.14340380669612823,\n",
      "      -0.00018164546592754072,\n",
      "      -0.28645369812934557,\n",
      "      -0.14466221625609652,\n",
      "      0.14302058959708913,\n",
      "      0.2853640679165013\n",
      "    ],\n",
      "    [\n",
      "      0.14028185844153007,\n",
      "      -0.003043977643320317,\n",
      "      0.0020456171458189537,\n",
      "      -0.14415316694583857,\n",
      "      0.001734521213559534,\n",
      "      -0.14257062006609195,\n",
      "      -0.1381031232015711,\n",
      "      -0.28438710976043213,\n",
      "      0.28355331958657926,\n",
      "      0.1381330887976599\n",
      "    ],\n",
      "    [\n",
      "      -0.14340838331355804,\n",
      "      -0.0031426239418157476,\n",
      "      0.003113789702623576,\n",
      "      0.1462632578467823,\n",
      "      -0.00356567121538702,\n",
      "      0.13766899896961637,\n",
      "      0.14457187270639318,\n",
      "      0.2878429791297711,\n",
      "      -0.2869384096058638,\n",
      "      -0.1445751742562162\n",
      "    ],\n",
      "    [\n",
      "      0.002800977990067524,\n",
      "      -0.14562490013528817,\n",
      "      0.14806065696668247,\n",
      "      0.0007109437833489468,\n",
      "      0.14544591969073462,\n",
      "      -0.0022774756520733686,\n",
      "      0.29088737762750505,\n",
      "      0.14455640249612445,\n",
      "      -0.1428880743260484,\n",
      "      -0.28973546686537394\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "linearNoBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "strategic-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "export {accuracy,Sigmoid,MSE,BinaryCrossEntropyLoss,CrossEntropyLoss,ReLU,Linear,Embedding,Learner}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
